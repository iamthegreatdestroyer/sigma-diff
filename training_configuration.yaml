# Phase 2 Training Configuration with Phase 1 Optimizations Enabled
# Generated for speedup validation

training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.001
  optimizer: AdamW
  warmup_steps: 500
  log_interval: 10
  checkpoint_interval: 100

# Model Architecture Configuration
# SimpleTransformerModel defaults from training_loop.py
model:
  vocab_size: 2048 # Token vocabulary size
  embedding_dim: 256 # Embedding dimension
  num_heads: 4 # Attention heads
  num_layers: 2 # Transformer layers
  ff_dim: 512 # Feed-forward dimension
  max_seq_len: 128 # Maximum sequence length

# Validation Configuration
validation:
  val_interval: 2 # Run validation every N epochs
  val_samples: 1024 # Number of validation samples
  patience: 5 # Early stopping patience

# Phase 1 Optimizations - DISABLED FOR DEBUGGING
kernel_optimizer:
  enabled: false
  cpu_optimization: true
  kernel_tiling: 32
  feature_detection: auto

semantic_compression:
  enabled: false
  block_size: 64
  compression_type: mrl # Multi-Resolution Layers
  sparsity_target: 0.7

inference_scaling:
  enabled: false
  cache_height: 128
  multi_path_reasoning: true

monitoring:
  metrics_collection: true
  real_time_logging: true
  checkpoint_format: pt

device:
  gpu: 0
  fallback_cpu: true
  fp16_enabled: false

# Optimization Orchestrator Settings
optimization_orchestrator:
  parameter_precedence: semantic_compression > kernel_optimizer > inference_scaling
  safety_gate_enforcement: true
  adaptive_adjustment: true

# Phase 1 Module Configuration - DISABLED FOR DEBUGGING
phase1_modules:
  kernel_optimizer:
    cpu_feature_detection: disabled
    kernel_tuning: disabled
    cache_optimization: disabled

  semantic_compression:
    mrl_encoding: disabled
    binary_embeddings: disabled
    sparse_attention: disabled

  inference_scaling:
    rlvr_enabled: false
    multi_path_inference: disabled
    kv_cache_optimization: disabled

# Success Criteria Thresholds (from Phase 2 validation)
success_criteria:
  training_speedup_target: 3.0 # 3.0x speedup target
  training_speedup_minimum: 2.8 # Minimum acceptable
  speedup_ramp_up_epochs: 5 # Expect full speedup by epoch 5
  memory_utilization_max: 0.85 # 85% max
  overhead_max_percent: 30 # Overhead max as % of speedup
  loss_must_decrease: true
  gradient_flow_healthy: true
