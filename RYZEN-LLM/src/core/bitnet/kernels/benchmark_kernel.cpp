/*
 * Phase 1 Day 2 Task A: BitNet Kernel Benchmarking & Performance Validation
 * [REF:ACO-101-D2A] - Measure actual GEMM kernel performance pre/post optimization
 * Validates 1.15-2.1x speedup target from kernel_optimizer.py tuning
 *
 * Usage: benchmark_kernel --matrix-size 1024 --repetitions 100 [--output report.json]
 */

#include <iostream>
#include <vector>
#include <chrono>
#include <cstring>
#include <cmath>
#include <algorithm>
#include <numeric>
#include <iomanip>
#include <fstream>
#include <sstream>
#include <thread>
#include <array>

#ifdef _WIN32
#include <intrin.h>
#else
#include <x86intrin.h>
#endif

// ============================================================================
// SIMD Capability Detection
// ============================================================================

class CPUCapabilities
{
public:
    struct Features
    {
        bool avx512f = false;
        bool avx512vnni = false;
        bool avx2 = false;
        bool fma = false;
        bool sse42 = false;
        int num_cores = 1;
        int cache_l3_kb = 8192; // Default 8MB
    };

    static Features detect()
    {
        Features feat;

        // CPUID leaf 1
        std::array<int, 4> cpuid_1 = cpuid(1);
        feat.sse42 = (cpuid_1[2] & (1 << 20)) != 0; // SSE4.2
        feat.fma = (cpuid_1[2] & (1 << 12)) != 0;   // FMA3
        feat.avx2 = (cpuid_1[2] & (1 << 28)) != 0;  // AVX (check ECX bit 28)

        // CPUID leaf 7 (extended features)
        std::array<int, 4> cpuid_7 = cpuid(7);
        feat.avx2 = (cpuid_7[1] & (1 << 5)) != 0;        // AVX2
        feat.avx512f = (cpuid_7[1] & (1 << 16)) != 0;    // AVX-512 Foundation
        feat.avx512vnni = (cpuid_7[2] & (1 << 11)) != 0; // AVX-512 VNNI

#ifdef _WIN32
                                                         // Windows: Use GetLogicalProcessorInformation
        feat.num_cores = std::thread::hardware_concurrency();
#else
                                                         // Linux: Parse /proc/cpuinfo (simulate with thread count)
        feat.num_cores = std::thread::hardware_concurrency();
#endif

        return feat;
    }

private:
    static std::array<int, 4> cpuid(int leaf)
    {
        std::array<int, 4> regs = {0, 0, 0, 0};
#ifdef _WIN32
        __cpuid(regs.data(), leaf);
#else
        asm("cpuid" : "=a"(regs[0]), "=b"(regs[1]), "=c"(regs[2]), "=d"(regs[3])
            : "a"(leaf));
#endif
        return regs;
    }
};

#include <thread>

// ============================================================================
// Matrix Operations - Baseline (Naive)
// ============================================================================

class BaselineGEMM
{
public:
    // C = alpha * A * B + beta * C (naive loop fusion)
    static void multiply(const float *A, const float *B, float *C,
                         int M, int N, int K, float alpha = 1.0f, float beta = 0.0f)
    {
        for (int i = 0; i < M; i++)
        {
            for (int j = 0; j < N; j++)
            {
                if (beta != 0.0f)
                    C[i * N + j] *= beta;
                else
                    C[i * N + j] = 0.0f;

                for (int k = 0; k < K; k++)
                {
                    C[i * N + j] += alpha * A[i * K + k] * B[k * N + j];
                }
            }
        }
    }
};

// ============================================================================
// Matrix Operations - AVX2 Optimized
// ============================================================================

class AVX2GEMM
{
public:
    // Tile-based GEMM with AVX2 (8 floats = 256 bits)
    static void multiply(const float *A, const float *B, float *C,
                         int M, int N, int K, float alpha = 1.0f, float beta = 0.0f)
    {
        const int TILE_M = 32;
        const int TILE_N = 32;
        const int TILE_K = 256;

        for (int ii = 0; ii < M; ii += TILE_M)
        {
            for (int jj = 0; jj < N; jj += TILE_N)
            {
                for (int kk = 0; kk < K; kk += TILE_K)
                {
                    // Compute tile
                    int m_end = std::min(ii + TILE_M, M);
                    int n_end = std::min(jj + TILE_N, N);
                    int k_end = std::min(kk + TILE_K, K);

                    multiply_tile_avx2(A, B, C, M, N, K,
                                       ii, jj, kk,
                                       m_end, n_end, k_end);
                }
            }
        }
    }

private:
    static void multiply_tile_avx2(const float *A, const float *B, float *C,
                                   int M, int N, int K,
                                   int i_start, int j_start, int k_start,
                                   int i_end, int j_end, int k_end)
    {
        for (int i = i_start; i < i_end; i++)
        {
            for (int j = j_start; j < j_end; j += 8)
            { // Process 8 floats at a time
                __m256 sum = _mm256_setzero_ps();

                for (int k = k_start; k < k_end; k++)
                {
                    __m256 a_val = _mm256_set1_ps(A[i * K + k]);
                    __m256 b_vals = _mm256_loadu_ps(&B[k * N + j]);
                    sum = _mm256_fmadd_ps(a_val, b_vals, sum);
                }

                _mm256_storeu_ps(&C[i * N + j], sum);
            }
        }
    }
};

// ============================================================================
// Matrix Operations - AVX-512 Optimized
// ============================================================================

class AVX512GEMM
{
public:
    // Tile-based GEMM with AVX-512 (16 floats = 512 bits)
    static void multiply(const float *A, const float *B, float *C,
                         int M, int N, int K, float alpha = 1.0f, float beta = 0.0f)
    {
        const int TILE_M = 64;
        const int TILE_N = 64;
        const int TILE_K = 1024;

        for (int ii = 0; ii < M; ii += TILE_M)
        {
            for (int jj = 0; jj < N; jj += TILE_N)
            {
                for (int kk = 0; kk < K; kk += TILE_K)
                {
                    int m_end = std::min(ii + TILE_M, M);
                    int n_end = std::min(jj + TILE_N, N);
                    int k_end = std::min(kk + TILE_K, K);

                    multiply_tile_avx512(A, B, C, M, N, K,
                                         ii, jj, kk,
                                         m_end, n_end, k_end);
                }
            }
        }
    }

private:
    static void multiply_tile_avx512(const float *A, const float *B, float *C,
                                     int M, int N, int K,
                                     int i_start, int j_start, int k_start,
                                     int i_end, int j_end, int k_end)
    {
#ifdef __AVX512F__
        for (int i = i_start; i < i_end; i++)
        {
            for (int j = j_start; j < j_end; j += 16)
            { // Process 16 floats at a time
                __m512 sum = _mm512_setzero_ps();

                for (int k = k_start; k < k_end; k++)
                {
                    __m512 a_val = _mm512_set1_ps(A[i * K + k]);
                    __m512 b_vals = _mm512_loadu_ps(&B[k * N + j]);
                    sum = _mm512_fmadd_ps(a_val, b_vals, sum);
                }

                _mm512_storeu_ps(&C[i * N + j], sum);
            }
        }
#else
        // Fallback to AVX2
        AVX2GEMM::multiply(A, B, C, M, N, K);
#endif
    }
};

// ============================================================================
// Benchmark Suite
// ============================================================================

struct BenchmarkResult
{
    std::string name;
    int matrix_size;
    int iterations;
    double total_time_ms;
    double avg_time_ms;
    double min_time_ms;
    double max_time_ms;
    double gflops; // Billion floating-point operations per second

    std::string to_json() const
    {
        std::ostringstream oss;
        oss << "{\n";
        oss << "  \"name\": \"" << name << "\",\n";
        oss << "  \"matrix_size\": " << matrix_size << ",\n";
        oss << "  \"iterations\": " << iterations << ",\n";
        oss << "  \"total_time_ms\": " << std::fixed << std::setprecision(2) << total_time_ms << ",\n";
        oss << "  \"avg_time_ms\": " << avg_time_ms << ",\n";
        oss << "  \"min_time_ms\": " << min_time_ms << ",\n";
        oss << "  \"max_time_ms\": " << max_time_ms << ",\n";
        oss << "  \"gflops\": " << std::setprecision(1) << gflops << "\n";
        oss << "}";
        return oss.str();
    }
};

class KernelBenchmark
{
public:
    KernelBenchmark(int matrix_size, int repetitions)
        : matrix_size_(matrix_size), repetitions_(repetitions) {}

    BenchmarkResult benchmark_baseline()
    {
        std::cout << "\nðŸ” Benchmarking Baseline (Naive Loops)..." << std::flush;

        float *A = allocate_matrix(matrix_size_);
        float *B = allocate_matrix(matrix_size_);
        float *C = allocate_matrix(matrix_size_);

        // Warm up
        for (int i = 0; i < 2; i++)
        {
            BaselineGEMM::multiply(A, B, C, matrix_size_, matrix_size_, matrix_size_);
        }

        std::vector<double> times;
        for (int i = 0; i < repetitions_; i++)
        {
            auto start = std::chrono::high_resolution_clock::now();
            BaselineGEMM::multiply(A, B, C, matrix_size_, matrix_size_, matrix_size_);
            auto end = std::chrono::high_resolution_clock::now();

            times.push_back(std::chrono::duration<double, std::milli>(end - start).count());
        }

        deallocate_matrix(A);
        deallocate_matrix(B);
        deallocate_matrix(C);

        return compute_statistics("baseline_naive", times);
    }

    BenchmarkResult benchmark_avx2()
    {
        std::cout << "\nðŸ” Benchmarking AVX2 Optimized..." << std::flush;

        float *A = allocate_matrix(matrix_size_);
        float *B = allocate_matrix(matrix_size_);
        float *C = allocate_matrix(matrix_size_);

        // Warm up
        for (int i = 0; i < 2; i++)
        {
            AVX2GEMM::multiply(A, B, C, matrix_size_, matrix_size_, matrix_size_);
        }

        std::vector<double> times;
        for (int i = 0; i < repetitions_; i++)
        {
            auto start = std::chrono::high_resolution_clock::now();
            AVX2GEMM::multiply(A, B, C, matrix_size_, matrix_size_, matrix_size_);
            auto end = std::chrono::high_resolution_clock::now();

            times.push_back(std::chrono::duration<double, std::milli>(end - start).count());
        }

        deallocate_matrix(A);
        deallocate_matrix(B);
        deallocate_matrix(C);

        return compute_statistics("avx2_optimized", times);
    }

    BenchmarkResult benchmark_avx512()
    {
        std::cout << "\nðŸ” Benchmarking AVX-512 Optimized..." << std::flush;

        float *A = allocate_matrix(matrix_size_);
        float *B = allocate_matrix(matrix_size_);
        float *C = allocate_matrix(matrix_size_);

        // Warm up
        for (int i = 0; i < 2; i++)
        {
            AVX512GEMM::multiply(A, B, C, matrix_size_, matrix_size_, matrix_size_);
        }

        std::vector<double> times;
        for (int i = 0; i < repetitions_; i++)
        {
            auto start = std::chrono::high_resolution_clock::now();
            AVX512GEMM::multiply(A, B, C, matrix_size_, matrix_size_, matrix_size_);
            auto end = std::chrono::high_resolution_clock::now();

            times.push_back(std::chrono::duration<double, std::milli>(end - start).count());
        }

        deallocate_matrix(A);
        deallocate_matrix(B);
        deallocate_matrix(C);

        return compute_statistics("avx512_optimized", times);
    }

    void generate_report(const std::vector<BenchmarkResult> &results)
    {
        std::cout << "\n"
                  << std::string(80, '=') << "\n";
        std::cout << "ðŸ“Š KERNEL BENCHMARK REPORT - PHASE 1 DAY 2 TASK A\n";
        std::cout << std::string(80, '=') << "\n\n";

        std::cout << "Hardware Configuration:\n";
        CPUCapabilities::Features features = CPUCapabilities::detect();
        std::cout << "  CPU Cores: " << features.num_cores << "\n";
        std::cout << "  AVX2: " << (features.avx2 ? "Yes" : "No") << "\n";
        std::cout << "  AVX-512: " << (features.avx512f ? "Yes" : "No") << "\n";
        std::cout << "  AVX-512 VNNI: " << (features.avx512vnni ? "Yes" : "No") << "\n";
        std::cout << "  L3 Cache: " << (features.cache_l3_kb / 1024) << "MB\n\n";

        std::cout << "Benchmark Results:\n";
        for (const auto &result : results)
        {
            std::cout << "\n  " << std::setw(25) << std::left << result.name << "\n";
            std::cout << "    Time (avg):   " << std::fixed << std::setprecision(2)
                      << result.avg_time_ms << " ms\n";
            std::cout << "    Time (min):   " << result.min_time_ms << " ms\n";
            std::cout << "    Time (max):   " << result.max_time_ms << " ms\n";
            std::cout << "    Performance: " << std::setprecision(1)
                      << result.gflops << " GFLOPS\n";
        }

        // Calculate speedups
        if (results.size() >= 2)
        {
            double speedup_baseline_to_avx2 = results[0].avg_time_ms / results[1].avg_time_ms;
            std::cout << "\n  Speedup (Baseline â†’ AVX2): " << std::fixed << std::setprecision(2)
                      << speedup_baseline_to_avx2 << "x\n";
        }

        if (results.size() >= 3)
        {
            double speedup_avx2_to_avx512 = results[1].avg_time_ms / results[2].avg_time_ms;
            double total_speedup = results[0].avg_time_ms / results[2].avg_time_ms;
            std::cout << "  Speedup (AVX2 â†’ AVX-512): " << speedup_avx2_to_avx512 << "x\n";
            std::cout << "  Total Speedup (Baseline â†’ AVX-512): " << total_speedup << "x\n";
            std::cout << "\n  ðŸŽ¯ TARGET VALIDATION: ";
            if (total_speedup >= 1.15)
            {
                std::cout << "âœ… PASSED (target: 1.15-2.1x, achieved: "
                          << total_speedup << "x)\n";
            }
            else
            {
                std::cout << "âš ï¸  BELOW TARGET (target: 1.15-2.1x, achieved: "
                          << total_speedup << "x)\n";
            }
        }

        std::cout << "\n"
                  << std::string(80, '=') << "\n";
    }

    void export_json(const std::vector<BenchmarkResult> &results, const std::string &output_path)
    {
        std::ofstream file(output_path);
        file << "{\n";
        file << "  \"benchmark\": \"kernel_optimization_validation\",\n";
        file << "  \"phase\": \"Phase 1 Day 2\",\n";
        file << "  \"task\": \"Task A - Kernel Benchmarking\",\n";
        file << "  \"matrix_size\": " << matrix_size_ << ",\n";
        file << "  \"iterations\": " << repetitions_ << ",\n";
        file << "  \"results\": [\n";

        for (size_t i = 0; i < results.size(); i++)
        {
            file << "    " << results[i].to_json();
            if (i < results.size() - 1)
                file << ",";
            file << "\n";
        }

        file << "  ]\n";
        file << "}\n";
        file.close();

        std::cout << "ðŸ’¾ Report exported to: " << output_path << "\n";
    }

private:
    int matrix_size_;
    int repetitions_;

    float *allocate_matrix(int size)
    {
        float *mat = new float[size * size];
        // Initialize with random values
        for (int i = 0; i < size * size; i++)
        {
            mat[i] = static_cast<float>(rand()) / RAND_MAX;
        }
        return mat;
    }

    void deallocate_matrix(float *mat)
    {
        delete[] mat;
    }

    BenchmarkResult compute_statistics(const std::string &name, const std::vector<double> &times)
    {
        double total = std::accumulate(times.begin(), times.end(), 0.0);
        double avg = total / times.size();
        double min_val = *std::min_element(times.begin(), times.end());
        double max_val = *std::max_element(times.begin(), times.end());

        // GFLOPS: (2 * M * N * K) FLOPs per multiplication
        // Matrix mult: MÃ—N result, K operations per element = 2*M*N*K FLOPs
        double flops = 2.0 * matrix_size_ * matrix_size_ * matrix_size_;
        double gflops = (flops / 1e9) / (avg / 1000.0); // avg is in ms

        return BenchmarkResult{name, matrix_size_, static_cast<int>(times.size()),
                               total, avg, min_val, max_val, gflops};
    }
};

// ============================================================================
// Main
// ============================================================================

int main()
{
    std::cout << "\n"
              << std::string(80, '=') << "\n";
    std::cout << "ðŸ¤– PHASE 1 DAY 2 TASK A: BITNET KERNEL BENCHMARKING\n";
    std::cout << "[REF:ACO-101-D2A] Validate 1.15-2.1x speedup target\n";
    std::cout << std::string(80, '=') << "\n";

    // Benchmark configuration
    int matrix_size = 1024; // 1024Ã—1024 matrices
    int repetitions = 100;  // 100 iterations per implementation

    KernelBenchmark bench(matrix_size, repetitions);

    std::cout << "\nðŸ“Š Running kernel benchmarks for " << matrix_size << "Ã—"
              << matrix_size << " matrices (" << repetitions << " reps each)...\n";

    std::vector<BenchmarkResult> results;
    results.push_back(bench.benchmark_baseline());
    results.push_back(bench.benchmark_avx2());
    results.push_back(bench.benchmark_avx512());

    // Generate report
    bench.generate_report(results);

    // Export to JSON
    bench.export_json(results, "kernel_benchmark_report.json");

    std::cout << "\nâœ… KERNEL BENCHMARK COMPLETE\n\n";

    return 0;
}
