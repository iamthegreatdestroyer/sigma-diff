# RYZEN-LLM

**CPU-First Large Language Model Infrastructure for AMD Ryzen Processors**

[REF:ES-001] - Executive Summary

## Overview

RYZEN-LLM is a high-performance LLM inference system designed specifically for AMD Ryzen CPUs, eliminating the need for expensive GPU hardware. By leveraging cutting-edge model architectures (BitNet b1.58, Mamba SSM, RWKV) and CPU-specific optimizations (AVX-512, VNNI, speculative decoding), RYZEN-LLM achieves efficient inference with quality comparable to traditional FP16 models.

### Key Features

- **ğŸš€ Efficient Inference**: 15-30 tokens/second on Ryzen 9, competitive with GPU-based solutions
- **ğŸ’¡ Novel Token Recycling**: Semantic compression and retrieval system for context reuse
- **ğŸ”§ Multi-Model Support**: BitNet (ternary), Mamba (linear SSM), RWKV (attention-free)
- **âš¡ CPU Optimizations**: AVX-512, VNNI, T-MAC lookup tables, speculative decoding
- **ğŸŒ OpenAI-Compatible API**: Drop-in replacement for existing workflows
- **ğŸ› ï¸ MCP Protocol**: External tool use and agent capabilities

## Quick Start

### Prerequisites

- AMD Ryzen 7000+ series (Zen 4) with AVX-512 support
- 16GB+ RAM (32GB recommended for Ryzen 9)
- Python 3.11+
- CMake 3.20+
- Docker (optional, for Qdrant)

### Installation

```bash
# Clone the repository
cd RYZEN-LLM

# Run setup script
./scripts/setup.sh

# Download models
python scripts/download_models.py --model bitnet-7b

# Start Qdrant vector database
docker run -d -p 6333:6333 qdrant/qdrant

# Start API server
python -m uvicorn src.api.server:app --host 0.0.0.0 --port 8000
```

### Docker Setup

```bash
cd RYZEN-LLM

# Build production image
docker build --target runtime -t ryzen-llm:latest .

# Run with volume mounts
docker run -d \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/storage:/app/storage \
  ryzen-llm:latest
```

## Usage

### API Example

```python
import openai

# Point to RYZEN-LLM server
openai.api_base = "http://localhost:8000/v1"

# Chat completion
response = openai.ChatCompletion.create(
    model="bitnet-7b",
    messages=[
        {"role": "user", "content": "Write a Python function to calculate fibonacci numbers"}
    ]
)

print(response.choices[0].message.content)
```

### cURL Example

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "bitnet-7b",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": true
  }'
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              API Layer (FastAPI)                    â”‚
â”‚  OpenAI-compatible REST API + MCP Protocol         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Model Orchestration Layer                   â”‚
â”‚  Multi-model routing, hot-loading, task classifier â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Token Recycling System                     â”‚
â”‚  RSU compression, vector storage, retrieval        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Optimization Layer                        â”‚
â”‚  V-Cache, AVX-512 SIMD, Speculative Decoding      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Core Inference Engines                  â”‚
â”‚  BitNet b1.58 | T-MAC | Mamba SSM | RWKV          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Supported Models

| Model | Size | Quantization | Context | Use Case |
|-------|------|--------------|---------|----------|
| BitNet 7B | 3.5GB | Ternary | 4K | Code, reasoning |
| BitNet 13B | 6.5GB | Ternary | 4K | Complex tasks |
| Mamba 2.8B | 5.6GB | FP16 | 8K | Chat, QA |
| RWKV 7B | 14GB | FP16 | 8K | Creative writing |
| Draft 350M | 350MB | INT8 | 2K | Speculative decoding |

## Performance

### Ryzen 9 7950X (16 cores, 64MB L3)

| Metric | BitNet 7B | Mamba 2.8B | RWKV 7B |
|--------|-----------|------------|---------|
| TTFT | 400ms | 350ms | 450ms |
| Tokens/sec | 25 | 35 | 22 |
| Memory | 8GB | 10GB | 16GB |
| Concurrent | 5 | 8 | 4 |

## Project Structure

```
RYZEN-LLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # C++ inference engines
â”‚   â”œâ”€â”€ optimization/      # CPU optimization layer
â”‚   â”œâ”€â”€ recycler/          # Token recycling system
â”‚   â”œâ”€â”€ orchestration/     # Model management
â”‚   â””â”€â”€ api/               # FastAPI server
â”œâ”€â”€ models/                # Model storage
â”œâ”€â”€ storage/               # RSU and cache storage
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ scripts/               # Utility scripts
â””â”€â”€ tests/                 # Test suites
```

## Documentation

- [Architecture](RYZEN-LLM/docs/architecture/README.md) - System design and components
- [API Documentation](RYZEN-LLM/docs/api/README.md) - REST API reference
- [Research Notes](RYZEN-LLM/docs/research/README.md) - Papers and insights

## Development

### Building C++ Components

```bash
mkdir build && cd build
cmake -G Ninja -DCMAKE_BUILD_TYPE=Release ..
ninja
```

### Running Tests

```bash
# Python tests
pytest tests/

# C++ tests (if enabled)
cd build && ctest
```

### Benchmarks

```bash
# Run all benchmarks
python scripts/benchmark.py --suite all --output results.json

# Specific suite
python scripts/benchmark.py --suite inference
```

## Hardware Requirements

### Minimum (Ryzen 7)
- Ryzen 7 7700X or better
- 16GB RAM
- 50GB disk space

### Recommended (Ryzen 9)
- Ryzen 9 7950X or better
- 32GB RAM
- 100GB SSD

### Optimal (Threadripper)
- Threadripper PRO 7975WX or better
- 64GB+ RAM
- 200GB NVMe SSD

## Roadmap

- [x] Project scaffolding
- [ ] Core inference engines implementation
- [ ] Token recycling system
- [ ] API server
- [ ] Model orchestration
- [ ] Optimization layer
- [ ] Docker deployment
- [ ] Benchmarks and evaluation
- [ ] Documentation completion

## Contributing

Contributions are welcome! Please see our contributing guidelines (coming soon).

## License

MIT License - see LICENSE file for details

## Acknowledgments

- BitNet b1.58: Microsoft Research
- Mamba: Tri Dao, Albert Gu
- RWKV: Bo Peng
- T-MAC: MIT CSAIL
- llama.cpp: Georgi Gerganov

## Contact

For questions and support, please open an issue on GitHub.

---

**RYZEN-LLM**: Making LLMs accessible on consumer CPUs
