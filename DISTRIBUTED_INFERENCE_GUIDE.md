# Distributed Inference Guide

## Overview

This guide covers the RYzen-LLM distributed inference system, which provides 4x speedup on 4 GPUs through tensor parallelism. The system implements advanced tensor parallel techniques for large language models.

## Architecture

### Core Components

1. **Tensor Parallelism Engine**

   - Row-wise and column-wise linear layer parallelism
   - Head-wise attention parallelism
   - Parallel MLP with SwiGLU activation

2. **Multi-GPU Orchestrator**

   - Process group management
   - Dynamic GPU allocation
   - Fault tolerance and recovery

3. **Distributed Model Loader**

   - Zero-copy checkpoint loading
   - Weight sharding and distribution
   - Memory-mapped loading for efficiency

4. **NCCL Communication Layer**
   - Optimized collective operations
   - Low-latency all-reduce and all-gather
   - Bandwidth-efficient data transfer

## Quick Start

### Prerequisites

```bash
# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers accelerate

# For distributed training (optional)
pip install deepspeed megatron-lm
```

### Basic Usage

```python
from src.distributed.orchestrator import MultiGPUOrchestrator
from src.distributed.tensor_parallel import create_tensor_parallel_config
from transformers import AutoModelForCausalLM, AutoTokenizer

# Initialize distributed environment
orchestrator = MultiGPUOrchestrator(
    rank=0,  # Your GPU rank (0-3)
    world_size=4,  # Total GPUs
    backend="nccl",
    device="cuda:0"
)

orchestrator.initialize()

# Load model with tensor parallelism
model_name = "microsoft/DialoGPT-medium"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Apply tensor parallelism
tp_config = create_tensor_parallel_config(
    world_size=4,
    rank=0,
    device=torch.device("cuda:0"),
    hidden_size=1024,  # Model hidden size
    intermediate_size=4096,  # MLP intermediate size
    num_attention_heads=16  # Number of attention heads
)

# Wrap model layers with tensor parallelism
# (Implementation depends on specific model architecture)

# Generate text
input_text = "Hello, how are you?"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda:0")

with torch.no_grad():
    outputs = model.generate(**inputs, max_length=50)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

## Advanced Configuration

### Custom Tensor Parallel Config

```python
from src.distributed.tensor_parallel import TensorParallelConfig

config = TensorParallelConfig(
    world_size=4,
    rank=0,
    device=torch.device("cuda:0"),
    input_size=4096,
    output_size=4096,
    bias=True,
    gather_output=True,
    init_method="normal"
)
```

### Multi-Node Setup

```python
# Master node (rank 0)
orchestrator = MultiGPUOrchestrator(
    rank=0,
    world_size=8,  # 2 nodes Ã— 4 GPUs
    backend="nccl"
)

orchestrator.initialize(
    master_addr="192.168.1.100",  # Master node IP
    master_port=29500
)

# Worker nodes (ranks 1-7)
orchestrator = MultiGPUOrchestrator(
    rank=1,  # Node 1, GPU 0
    world_size=8,
    backend="nccl"
)

orchestrator.initialize(
    master_addr="192.168.1.100",
    master_port=29500
)
```

## Performance Optimization

### Memory Optimization

```python
# Use gradient checkpointing
model.gradient_checkpointing_enable()

# Use mixed precision
from torch.cuda.amp import autocast

with autocast():
    outputs = model(inputs)

# Use memory-mapped loading
loader = DistributedCheckpointLoader(
    checkpoint_dir="/path/to/checkpoints",
    rank=rank,
    world_size=world_size,
    use_memory_map=True,
    enable_prefetch=True
)
```

### Communication Optimization

```python
# Use optimized NCCL communicator
communicator = NCCLCommunicator()
communicator.optimize_for_bandwidth()  # Maximize bandwidth utilization

# Batch communication operations
with communicator.batch_operations():
    # Multiple tensor operations here
    # Automatically batched for efficiency
    pass
```

### Benchmarking

```python
from performance_benchmarks import DistributedInferenceBenchmark, BenchmarkConfig

config = BenchmarkConfig(
    model_size="7B",
    batch_size=8,
    seq_len=512,
    num_layers=12,
    num_attention_heads=32,
    hidden_size=4096,
    intermediate_size=11008,
    world_size=4
)

benchmark = DistributedInferenceBenchmark(config)
results = benchmark.run_comprehensive_benchmark()

print(f"Latency P50: {results.latency_p50:.2f}ms")
print(f"Throughput: {results.throughput:.0f} tokens/sec")
print(f"Speedup: {results.speedup_factor:.2f}x")
```

## Model Compatibility

### Supported Architectures

- **Transformer-based models**: GPT, BERT, T5, LLaMA
- **Attention mechanisms**: Multi-head, Grouped-query attention (GQA)
- **Activation functions**: ReLU, GELU, SwiGLU, GeGLU

### Model Size Guidelines

| Model Size | GPUs Required | Memory per GPU | Expected Speedup |
| ---------- | ------------- | -------------- | ---------------- |
| 7B params  | 4             | 8-12 GB        | 3.8-4.2x         |
| 13B params | 4             | 16-20 GB       | 3.8-4.2x         |
| 30B params | 8             | 20-24 GB       | 3.5-4.0x         |
| 65B params | 16            | 24-28 GB       | 3.2-3.8x         |

## Troubleshooting

### Common Issues

#### 1. NCCL Communication Errors

```python
# Check NCCL installation
python -c "import torch; print(torch.cuda.nccl.is_available())"

# Verify GPU connectivity
nvidia-smi topo -m

# Use TCP instead of IB for testing
export NCCL_IB_DISABLE=1
```

#### 2. Memory Issues

```python
# Monitor memory usage
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

# Use gradient accumulation
accumulation_steps = 4
for step in range(accumulation_steps):
    # Forward and accumulate gradients
    pass
```

#### 3. Synchronization Issues

```python
# Add explicit synchronization
torch.cuda.synchronize()

# Check process group health
orchestrator.check_health()

# Attempt recovery
if not orchestrator.check_health():
    orchestrator.attempt_recovery()
```

### Performance Tuning

#### Profiling

```python
# Use PyTorch profiler
with torch.profiler.profile() as prof:
    # Your inference code
    pass

prof.export_chrome_trace("trace.json")

# Use custom performance monitor
stats = orchestrator.get_performance_stats()
print(f"Communication time: {stats['communication_time']['mean']:.3f}s")
```

#### Optimization Checklist

- [ ] Use appropriate batch size for your GPU memory
- [ ] Enable gradient checkpointing for large models
- [ ] Use mixed precision (FP16/BF16)
- [ ] Optimize NCCL buffer sizes
- [ ] Use pinned memory for data loading
- [ ] Profile and eliminate bottlenecks

## Monitoring and Observability

### Metrics Collection

```python
# Enable monitoring
orchestrator = MultiGPUOrchestrator(
    rank=rank,
    world_size=world_size,
    enable_monitoring=True
)

# Get performance stats
stats = orchestrator.get_performance_stats()
print(f"Uptime: {stats['uptime_seconds']:.0f}s")
print(f"Failures: {stats['failure_count']}")
```

### Logging Configuration

```python
import logging

# Configure distributed logging
logging.basicConfig(
    level=logging.INFO,
    format=f'Rank {rank}: %(asctime)s - %(levelname)s - %(message)s'
)

# Log to file
handler = logging.FileHandler(f'distributed_rank_{rank}.log')
logger.addHandler(handler)
```

## Production Deployment

### Docker Setup

```dockerfile
FROM nvidia/cuda:12.1-runtime-ubuntu20.04

# Install Python and dependencies
RUN apt-get update && apt-get install -y python3 python3-pip
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application
COPY src/ /app/src/
COPY performance_benchmarks.py /app/

# Set environment
ENV CUDA_VISIBLE_DEVICES=0,1,2,3
ENV NCCL_IB_DISABLE=1

# Run distributed inference
CMD ["python", "-m", "torch.distributed.launch", \
     "--nproc_per_node=4", \
     "src/distributed/inference_server.py"]
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: distributed-llm-inference
spec:
  replicas: 1 # One pod with 4 containers
  template:
    spec:
      containers:
        - name: gpu-0
          image: distributed-llm:latest
          env:
            - name: RANK
              value: "0"
            - name: WORLD_SIZE
              value: "4"
          resources:
            limits:
              nvidia.com/gpu: 1
        - name: gpu-1
          image: distributed-llm:latest
          env:
            - name: RANK
              value: "1"
            - name: WORLD_SIZE
              value: "4"
          resources:
            limits:
              nvidia.com/gpu: 1
      # Add containers for GPU 2 and 3
```

## API Reference

### MultiGPUOrchestrator

```python
class MultiGPUOrchestrator:
    def __init__(self, rank: int, world_size: int, backend: str = "nccl",
                 device: Optional[str] = None, enable_monitoring: bool = True)
    def initialize(self, master_addr: str = "127.0.0.1", master_port: int = 29500) -> None
    def barrier(self) -> None
    def get_rank(self) -> int
    def get_world_size(self) -> int
    def get_device(self) -> torch.device
    def is_master(self) -> bool
    def cleanup(self) -> None
    def check_health(self) -> bool
    def attempt_recovery(self) -> bool
    def get_performance_stats(self) -> Dict[str, Any]
```

### TensorParallelConfig

```python
@dataclass
class TensorParallelConfig:
    world_size: int
    rank: int
    device: torch.device
    input_size: int
    output_size: int
    bias: bool = True
    gather_output: bool = True
    init_method: str = 'normal'
```

## Contributing

### Code Style

```python
# Use type hints
def process_batch(input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]) -> torch.Tensor:
    pass

# Use docstrings
def distributed_forward(self, x: torch.Tensor) -> torch.Tensor:
    """Forward pass with tensor parallelism.

    Args:
        x: Input tensor

    Returns:
        Output tensor
    """
    pass
```

### Testing

```bash
# Run unit tests
python -m pytest tests/test_distributed_inference.py -v

# Run performance benchmarks
python performance_benchmarks.py

# Run integration tests
python -m pytest tests/ -k "integration" --tb=short
```

## Support

### Getting Help

1. Check the troubleshooting section above
2. Review the test suite for examples
3. Check performance benchmarks for optimization tips
4. File an issue with detailed reproduction steps

### Performance Targets

- **Latency**: P99 < 50ms for 1K tokens
- **Throughput**: 1000+ tokens/second
- **Speedup**: 3.8-4.2x on 4 GPUs
- **Memory**: <10% communication overhead

### Compatibility

- **PyTorch**: 2.0+
- **CUDA**: 12.1+
- **NCCL**: 2.16+
- **Python**: 3.8+

---

_This guide covers the complete distributed inference system. For detailed API documentation, see the docstrings in the source code._
