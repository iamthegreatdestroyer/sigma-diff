# Phase 2 Pre-Execution Strategy: Critical Path Optimization

**Date:** February 9, 2026  
**Status:** ğŸŸ¢ Ready for Parallel Execution  
**Critical Path:** 6-8 hours to complete all pre-Day-1 validations

---

## ğŸ¯ Executive Summary

This document outlines the **critical path sequence** for completing all 5 pre-Day-1 tasks with **parallel execution** where dependencies allow. Tasks are organized into 3 sequential phases with multiple parallel workstreams.

---

## ğŸ“Š Dependency Graph

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: CRITICAL VALIDATION (BLOCKING)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ Task 2: API Validation (APEX)                              â”‚
â”‚ â”œâ”€ Review kernel_optimizer.py signatures                   â”‚
â”‚ â”œâ”€ Review semantic_compression.py interfaces               â”‚
â”‚ â”œâ”€ Test Phase 1 module imports                             â”‚
â”‚ â””â”€ Generate PHASE1_API_REFERENCE.md                        â”‚
â”‚                                                              â”‚
â”‚ â±ï¸  DURATION: 2 hours                                       â”‚
â”‚ â›” BLOCKER: Yes (unblocks phases 2-5)                       â”‚
â”‚ ğŸ”— DEPENDS ON: None                                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: PARALLEL EXECUTION (Starts after Phase 1)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ Task 4: CI/CD Setup (FLUX)          Task 5: Profiling      â”‚
â”‚ â”œâ”€ Create training_ci.yml            (VELOCITY)            â”‚
â”‚ â”œâ”€ Setup GPU runner config            â”œâ”€ Run baseline      â”‚
â”‚ â”œâ”€ Configure artifact storage          â”‚   profiling        â”‚
â”‚ â””â”€ Fix Ubuntu GTest (optional)        â”œâ”€ Measure overhead  â”‚
â”‚                                        â””â”€ Report results    â”‚
â”‚ â±ï¸  2 hours (can overlap with       â”‚                      â”‚
â”‚     Task 5)                           â±ï¸  1-2 hours       â”‚
â”‚                                       ğŸ”— DEPENDS: Phase 1  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“ (Parallel)          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                     â”‚
        â†“                    â†“                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task 1:          â”‚ â”‚ Task 3:          â”‚  â”‚ DELIVERABLES â”‚
â”‚ OptOrchestrator  â”‚ â”‚ Test Suite       â”‚  â”‚              â”‚
â”‚ (ARCHITECT+APEX) â”‚ â”‚ (ECLIPSE+VELOCITY)â”‚  â”‚ âœ… CI/CD    â”‚
â”‚                  â”‚ â”‚                  â”‚  â”‚ âœ… Overhead  â”‚
â”‚ â”œâ”€ Orchestrator  â”‚ â”‚ â”œâ”€ Unit tests    â”‚  â”‚ âœ… Orchestr. â”‚
â”‚ â”‚  class         â”‚ â”‚ â”œâ”€ Integration   â”‚  â”‚ âœ… Tests     â”‚
â”‚ â”œâ”€ Param rules   â”‚ â”‚ â”œâ”€ Accuracy val. â”‚  â”‚ âœ… API Ref   â”‚
â”‚ â”œâ”€ Safety gates  â”‚ â”‚ â””â”€ Coverage      â”‚  â”‚              â”‚
â”‚ â””â”€ Design docs   â”‚ â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                  â”‚ â”‚ â±ï¸  3-4 hours   â”‚
â”‚ â±ï¸  2-3 hours    â”‚ â”‚ ğŸ”— DEPENDS:     â”‚
â”‚ ğŸ”— DEPENDS:      â”‚ â”‚    Phase 1 + T1 â”‚
â”‚    Phase 1 + T4  â”‚ â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Execution Phases

### **PHASE 1: CRITICAL VALIDATION (Sequential)** âš ï¸ BLOCKER

**Duration:** 2 hours  
**Owner:** @APEX  
**Status:** ğŸ”´ NOT STARTED

#### Task 2: API Validation

**Objectives:**

1. âœ… Review Phase 1 module function signatures
2. âœ… Test all imports work correctly
3. âœ… Generate API reference documentation
4. âœ… Identify any integration issues early

**Execution Steps:**

```python
# STEP 1: Validate Imports
cd s:\Ryot\RYZEN-LLM\scripts
python -c "
from kernel_optimizer import detect_and_tune
from semantic_compression import encode, decode
from inference_scaling import optimize_kv_cache
print('âœ… All Phase 1 modules importable')
"

# STEP 2: Inspect Function Signatures
python << 'EOF'
import inspect
from kernel_optimizer import detect_and_tune
from semantic_compression import encode, decode
from inference_scaling import optimize_kv_cache

apis = {
    'kernel_optimizer.detect_and_tune': detect_and_tune,
    'semantic_compression.encode': encode,
    'semantic_compression.decode': decode,
    'inference_scaling.optimize_kv_cache': optimize_kv_cache,
}

for name, func in apis.items():
    sig = inspect.signature(func)
    print(f"{name}{sig}")
EOF

# STEP 3: Test with dummy inputs
python << 'EOF'
import torch
from kernel_optimizer import detect_and_tune
from semantic_compression import encode, decode

# Test kernel optimizer
result = detect_and_tune(activations=torch.randn(1, 1024), model='test')
print(f"kernel_optimizer result type: {type(result)}")

# Test compression
encoded = encode(torch.randn(1, 1024))
decoded = decode(encoded)
print(f"compression round-trip successful: {decoded.shape}")
EOF
```

**Deliverables:**

- [ ] `docs/PHASE1_API_REFERENCE.md` with exact signatures
- [ ] Integration test passing
- [ ] Any API modification requests documented

**Success Criteria:** âœ… All Phase 1 modules importable + signatures documented

---

### **PHASE 2: PARALLEL EXECUTION** (Starts after Phase 1) ğŸš€

**Duration:** 2-4 hours (parallel)  
**Can Start:** Immediately after Phase 1 completes

#### Task 4: Training CI/CD Setup [FLUX]

**Parallel with:** Tasks 5  
**Duration:** 2 hours  
**Owner:** @FLUX

**Objectives:**

1. Create GPU-capable training workflow
2. Configure artifact storage for checkpoints
3. Fix Ubuntu build issue (confidence gain)

**Execution:**

```yaml
# Create: .github/workflows/training_ci.yml
name: Training Pipeline
on:
  push:
    branches: [sprint6/api-integration, main]
  pull_request:

jobs:
  train:
    runs-on: [self-hosted, gpu] # Requires GPU runner
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install PyTorch with CUDA
        run: |
          pip install torch torchvision torchaudio \
            --index-url https://download.pytorch.org/whl/cu118

      - name: Install dependencies
        run: |
          pip install -r RYZEN-LLM/requirements-training.txt

      - name: Run training baseline
        run: |
          python RYZEN-LLM/scripts/training_loop.py \
            --config RYZEN-LLM/configs/training_configuration.yaml \
            --epochs 5 \
            --output-dir ./checkpoints

      - name: Upload checkpoints
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: training-checkpoints
          path: "./checkpoints/**"
          retention-days: 7

      - name: Upload metrics
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: training-metrics
          path: "./metrics/**"
          retention-days: 7
```

**Deliverables:**

- [ ] `.github/workflows/training_ci.yml` created
- [ ] `RYZEN-LLM/requirements-training.txt` with all deps
- [ ] Ubuntu GTest issue root-caused and fixed
- [ ] CI workflow tested on feature branch

**Success Criteria:** âœ… CI workflow runs successfully on GPU runner

---

#### Task 5: Overhead Profiling Baseline [VELOCITY]

**Parallel with:** Task 4  
**Duration:** 1-2 hours  
**Owner:** @VELOCITY  
**Depends on:** Phase 1 API validation

**Objectives:**

1. Measure individual optimization overhead costs
2. Profile Phase 1 modules in isolation
3. Generate baseline overhead report
4. Validate net speedup feasibility

**Execution:**

```python
# Create: RYZEN-LLM/scripts/benchmark_overhead.py
import time
import torch
import json
from typing import Dict, Tuple

class OverheadProfiler:
    def __init__(self, device='cuda'):
        self.device = device
        self.results = {}

    def profile_kernel_optimizer(self, iterations=100) -> Dict[str, float]:
        """Measure kernel optimization detection overhead"""
        from kernel_optimizer import detect_and_tune

        activations = torch.randn(32, 1024, device=self.device)

        # Warmup
        for _ in range(5):
            result = detect_and_tune(activations)

        # Timer
        start = time.perf_counter()
        for _ in range(iterations):
            result = detect_and_tune(activations)
        duration = (time.perf_counter() - start) / iterations

        return {
            'operation': 'kernel_optimizer.detect_and_tune',
            'avg_time_ms': duration * 1000,
            'iterations': iterations,
            'overhead_percent': (duration / 0.01) * 100  # Assume 10ms baseline
        }

    def profile_compression(self, iterations=100) -> Dict[str, float]:
        """Measure compression encode/decode overhead"""
        from semantic_compression import encode, decode

        activations = torch.randn(32, 1024, device=self.device)

        # Warmup
        for _ in range(5):
            enc = encode(activations)
            dec = decode(enc)

        # Timer
        start = time.perf_counter()
        for _ in range(iterations):
            enc = encode(activations)
            dec = decode(enc)
        duration = (time.perf_counter() - start) / iterations

        return {
            'operation': 'compression.encode+decode',
            'avg_time_ms': duration * 1000,
            'iterations': iterations,
            'overhead_percent': (duration / 0.01) * 100
        }

    def profile_rlvr(self, iterations=100) -> Dict[str, float]:
        """Measure RLVR path selection overhead"""
        from inference_scaling import optimize_kv_cache

        cache = torch.randn(1, 32, 1024, 64, device=self.device)

        # Warmup
        for _ in range(5):
            result = optimize_kv_cache(cache)

        # Timer
        start = time.perf_counter()
        for _ in range(iterations):
            result = optimize_kv_cache(cache)
        duration = (time.perf_counter() - start) / iterations

        return {
            'operation': 'inference_scaling.optimize_kv_cache',
            'avg_time_ms': duration * 1000,
            'iterations': iterations,
            'overhead_percent': (duration / 0.01) * 100
        }

    def run_all(self) -> Dict:
        """Run all profiling benchmarks"""
        print("ğŸ”„ Starting overhead profiling...")

        self.results['kernel_optimizer'] = self.profile_kernel_optimizer()
        print(f"âœ… Kernel optimizer: {self.results['kernel_optimizer']['avg_time_ms']:.2f}ms")

        self.results['compression'] = self.profile_compression()
        print(f"âœ… Compression: {self.results['compression']['avg_time_ms']:.2f}ms")

        self.results['rlvr'] = self.profile_rlvr()
        print(f"âœ… RLVR: {self.results['rlvr']['avg_time_ms']:.2f}ms")

        return self.results

    def save_report(self, output_file='reports/overhead_analysis.json'):
        """Save overhead analysis report"""
        import os
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, 'w') as f:
            json.dump(self.results, f, indent=2)

        # Print summary
        print(f"\nğŸ“Š Overhead Analysis Report:")
        for op, data in self.results.items():
            print(f"  {op}: {data['avg_time_ms']:.2f}ms ({data['overhead_percent']:.1f}% overhead)")

if __name__ == '__main__':
    profiler = OverheadProfiler()
    results = profiler.run_all()
    profiler.save_report()
```

**Deliverables:**

- [ ] `RYZEN-LLM/scripts/benchmark_overhead.py` created
- [ ] `reports/overhead_analysis.json` generated
- [ ] Baseline overhead measured for each optimization
- [ ] Net speedup feasibility validated

**Success Criteria:** âœ… Overhead < 30% of gross speedup (net speedup positive)

---

### **PHASE 3: SEQUENTIAL DEPENDENT TASKS** ğŸ¯

**Duration:** 4-6 hours total (can overlap)

#### Task 1: OptimizationOrchestrator Implementation [ARCHITECT + APEX]

**Depends on:** Phase 1 (API validation) + Phase 2/Task 4 (some deliverables)  
**Duration:** 2-3 hours  
**Can Start:** Immediately after Phase 1 + Phase 2

**Objectives:**

1. Design OptimizationOrchestrator class
2. Implement parameter precedence rules
3. Create safety gates for optimization interaction
4. Document architecture with diagrams

**Implementation:**

```python
# Create: RYZEN-LLM/src/optimization_orchestrator.py
from typing import Dict, List, Tuple
from dataclasses import dataclass
import torch

@dataclass
class OptimizationState:
    """Snapshots optimization configuration at each checkpoint"""
    kernel_config: Dict
    compression_config: Dict
    rlvr_config: Dict
    epoch: int
    timestamp: float

class OptimizationOrchestrator:
    """
    Coordinates interaction between 3 independent optimizations:
    1. Kernel optimization (device-level)
    2. Semantic compression (data encoding)
    3. RLVR inference scaling (algorithm-level)
    """

    def __init__(self, config: Dict):
        self.config = config
        self.optimization_states: List[OptimizationState] = []
        self.parameter_precedence = {
            'compression_ratio': 'kernel_tile_size',  # Compression takes precedence
            'kernel_tile_size': 'rlvr_path_depth',    # Kernel takes precedence
        }

    def detect_parameter_conflicts(self,
                                   kernel_cfg: Dict,
                                   compression_cfg: Dict,
                                   rlvr_cfg: Dict) -> List[str]:
        """Identify potential parameter conflicts between optimizations"""
        conflicts = []

        # Conflict 1: Kernel tile size vs compression block size alignment
        k_tile = kernel_cfg.get('tile_size', 128)
        c_block = compression_cfg.get('block_size', 256)

        if k_tile % 64 != 0 or c_block % 64 != 0:
            conflicts.append(
                f"âš ï¸ Unaligned tile sizes: kernel={k_tile}, compression={c_block} "
                f"(should align to cache line for efficiency)"
            )

        # Conflict 2: Compression ratio too aggressive
        c_ratio = compression_cfg.get('compression_ratio', 30)
        if c_ratio > 50:
            conflicts.append(
                f"âš ï¸ Aggressive compression ratio {c_ratio}x may cause "
                f"gradient degradation (consider reducing below 50x)"
            )

        # Conflict 3: RLVR scaling incompatible with low compression
        if c_ratio < 10:
            conflicts.append(
                f"âš ï¸ Low compression ({c_ratio}x) may not benefit from RLVR "
                f"path pruning (consider increasing compression ratio)"
            )

        return conflicts

    def apply_precedence_rules(self,
                              kernel_cfg: Dict,
                              compression_cfg: Dict) -> Dict:
        """
        Apply parameter precedence rules to resolve conflicts.
        PRECEDENCE ORDER:
        1. Compression ratio (data encoding takes priority)
        2. Kernel tile size (device tuning second)
        3. RLVR path depth (algorithm level third)
        """
        adjusted = {
            'kernel_config': kernel_cfg.copy(),
            'compression_config': compression_cfg.copy(),
        }

        # If compression ratio high, ensure kernel tile is aligned
        c_ratio = compression_cfg.get('compression_ratio', 30)
        if c_ratio > 40:
            # Align kernel tile to compression block boundaries
            adjusted['kernel_config']['tile_size'] = 256
            print(f"ğŸ”§ Adjusted kernel tile_size â†’ 256 (compression-aligned)")

        return adjusted

    def validate_safety_gates(self, metrics: Dict) -> Tuple[bool, List[str]]:
        """
        Safety gates to ensure optimization doesn't break training:
        1. Loss must be finite (no NaNs/Infs)
        2. Gradient magnitude must be reasonable
        3. Compression reconstruction error within threshold
        """
        warnings = []
        safe = True

        # Gate 1: Check loss validity
        if torch.isnan(metrics.get('loss', 0)) or torch.isinf(metrics.get('loss', 0)):
            warnings.append("ğŸ”´ Loss contains NaN/Inf - UNSAFE")
            safe = False

        # Gate 2: Check gradient flow
        grad_norm = metrics.get('gradient_norm', 1.0)
        if grad_norm > 10.0:
            warnings.append(f"âš ï¸ Gradient norm {grad_norm:.2f} (potential instability)")
        elif grad_norm < 1e-4:
            warnings.append(f"âš ï¸ Gradient norm {grad_norm:.2e} (potential vanishing gradients)")

        # Gate 3: Check compression reconstruction error
        recon_error = metrics.get('compression_recon_error', 0)
        if recon_error > 0.05:
            warnings.append(f"âš ï¸ Reconstruction error {recon_error:.4f} (consider reducing compression)")

        return safe, warnings

    def snapshot_configuration(self, epoch: int, timestamp: float):
        """Snapshot optimization configuration for reproducibility"""
        state = OptimizationState(
            kernel_config=self.config['kernel_optimizer'],
            compression_config=self.config['semantic_compression'],
            rlvr_config=self.config['inference_scaling'],
            epoch=epoch,
            timestamp=timestamp
        )
        self.optimization_states.append(state)
        print(f"ğŸ“¸ Configuration snapshot at epoch {epoch}")

    def get_checkpoint_metadata(self) -> Dict:
        """Return metadata for model checkpoint"""
        return {
            'optimization_orchestrator_version': '1.0',
            'configurations_count': len(self.optimization_states),
            'latest_config': {
                'kernel': self.config['kernel_optimizer'],
                'compression': self.config['semantic_compression'],
                'rlvr': self.config['inference_scaling'],
            }
        }
```

**Deliverables:**

- [ ] `src/optimization_orchestrator.py` implemented
- [ ] Parameter precedence rules documented
- [ ] Safety gates functional
- [ ] Architecture documentation with UML/diagrams

**Success Criteria:** âœ… Orchestrator prevents parameter conflicts + passes unit tests

---

#### Task 3: Comprehensive Test Suite [ECLIPSE + VELOCITY]

**Depends on:** Phase 1 + Task 1 (OptimizationOrchestrator architecture)  
**Duration:** 3-4 hours  
**Can Start:** Once Task 1 is sketched (parallel partial implementation)

**Objectives:**

1. Create unit tests for all components
2. Create integration tests for optimization combinations
3. Define accuracy validation methodology
4. Establish reproducibility validation

**Structure:**

```python
# tests/test_training_loop.py - Unit Tests
import pytest
import torch
from training_loop import TrainingLoop

class TestTrainingLoop:
    @pytest.fixture
    def training_loop(self):
        config = {
            'model_name': 'tiny-1b',
            'batch_size': 32,
            'learning_rate': 1e-4,
        }
        return TrainingLoop(config=config)

    def test_training_step_output_shape(self, training_loop):
        """Validate training step produces correct output shapes"""
        batch = {
            'input_ids': torch.randint(0, 1000, (32, 128)),
            'attention_mask': torch.ones(32, 128),
        }
        logits, loss = training_loop.training_step(batch)
        assert logits.shape == (32, 128, 1024)  # vocab size
        assert loss.shape == ()

    def test_loss_computation_correctness(self, training_loop):
        """Validate loss computation is correct"""
        logits = torch.randn(32, 128, 1024)
        targets = torch.randint(0, 1024, (32, 128))
        loss = training_loop.compute_loss(logits, targets)
        assert torch.isfinite(loss)
        assert loss > 0

# tests/test_integration.py - Integration Tests
class TestOptimizationIntegration:
    def test_kernel_plus_compression(self):
        """Test kernel optimization + compression compatibility"""
        # Load Phase 1 modules
        from kernel_optimizer import detect_and_tune
        from semantic_compression import encode, decode

        # Create dummy activation
        activation = torch.randn(1, 1024)

        # Apply kernel optimization
        opt_activation = detect_and_tune(activation)

        # Apply compression
        encoded = encode(opt_activation)
        decoded = decode(encoded)

        # Verify round-trip
        assert decoded.shape == opt_activation.shape
        error = torch.mean((opt_activation - decoded) ** 2)
        assert error < 0.1  # Reconstruction error threshold

# tests/test_accuracy_validation.py - Accuracy Tests
class TestAccuracyValidation:
    def test_baseline_vs_optimized_convergence(self):
        """Validate optimized training maintains baseline accuracy"""
        # Train baseline model
        baseline_model = train_baseline(epochs=5)
        baseline_accuracy = evaluate(baseline_model)

        # Train optimized model
        optimized_model = train_optimized(epochs=5)
        optimized_accuracy = evaluate(optimized_model)

        # Check accuracy within threshold (99% of baseline)
        accuracy_ratio = optimized_accuracy / baseline_accuracy
        assert accuracy_ratio >= 0.99
        print(f"âœ… Optimized model maintains {accuracy_ratio*100:.1f}% of baseline accuracy")

# success_criteria.json
{
  "speedup_metrics": {
    "target_training_speedup": "3-5x per epoch",
    "target_inference_speedup_ttft": "2.5-3.5x",
    "target_inference_speedup_throughput": "40-60 tokens/sec",
    "measurement": "wall_time_per_epoch"
  },
  "accuracy_metrics": {
    "baseline_accuracy": "measure_before_optimization",
    "optimized_accuracy": "must_be >= 99% of baseline",
    "test_datasets": ["validation_set", "test_set"]
  },
  "memory_metrics": {
    "target_memory_reduction": "40%",
    "max_peak_memory": "VRAM_limit * 0.9"
  },
  "convergence_metrics": {
    "max_loss_divergence": "< 5% from baseline",
    "gradient_stability": "norm must be 1e-6 to 10",
    "training_stability": "no NaN/Inf in any step"
  }
}
```

**Deliverables:**

- [ ] `tests/test_training_loop.py` (12-15 unit tests)
- [ ] `tests/test_integration.py` (8-10 integration tests)
- [ ] `tests/test_accuracy_validation.py` (5-6 tests)
- [ ] `success_criteria.json` with exact thresholds
- [ ] Test coverage report (>80%)

**Success Criteria:** âœ… All tests passing, >80% code coverage

---

## ğŸ“ˆ Timeline Summary

```
09-FEB-2026 (TODAY):

PHASE 1 - 14:00-16:00 (2 hours)
â”œâ”€ Task 2: API Validation (@APEX)
â””â”€ âœ… DELIVERABLES:
   â””â”€ docs/PHASE1_API_REFERENCE.md

PHASE 2 - 16:00-19:00 (3 hours, parallel)
â”œâ”€ Task 4: CI/CD Setup (@FLUX) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”œâ”€ Task 5: Overhead Profiling (@VELOCITY)  â”‚ RUN IN PARALLEL
â””â”€ âœ… DELIVERABLES:                         â”‚
   â”œâ”€ .github/workflows/training_ci.yml    â”‚
   â”œâ”€ requirements-training.txt            â”‚
   â””â”€ reports/overhead_analysis.json       â”‚
                                           â”‚
PHASE 3 - 19:00-23:00 (4 hours)           â”‚
â”œâ”€ Task 1: OptOrchestrator (ARCH+APEX) â†â”€â”€â”´â”€ (depends on 2,4)
â”‚  âœ… src/optimization_orchestrator.py
â”‚
â”œâ”€ Task 3: Test Suite (ECLIPSE+VEL) â† (depends on 1)
â”‚  âœ… tests/test_*.py files
â”‚  âœ… success_criteria.json

23:00 â†’ READY FOR PHASE 2 EXECUTION ON DAY 1
```

---

## ğŸš€ Execution Commands

### Phase 1: API Validation

```bash
# Run by: @APEX
cd s:\Ryot
python RYZEN-LLM/scripts/validate_phase1_apis.py

# Expected output:
# âœ… kernel_optimizer.detect_and_tune imported
# âœ… semantic_compression.encode/decode imported
# âœ… inference_scaling.optimize_kv_cache imported
# ğŸ“„ docs/PHASE1_API_REFERENCE.md generated
```

### Phase 2: Parallel Tasks

```bash
# Task 4 (CI/CD): Run by @FLUX
cp phase2_workflows/training_ci.yml .github/workflows/training_ci.yml
git add .github/workflows/training_ci.yml
git commit -m "feat: Add GPU training CI/CD workflow"

# Task 5 (Profiling): Run by @VELOCITY
python RYZEN-LLM/scripts/benchmark_overhead.py
# Output: reports/overhead_analysis.json
```

### Phase 3: Dependent Tasks

```bash
# Task 1 (OptOrchestrator): Run by @ARCHITECT + @APEX
# Implementation follows template above

# Task 3 (Tests): Run by @ECLIPSE + @VELOCITY
pytest tests/ -v --cov=src
# Must achieve >80% coverage
```

---

## âœ… Success Criteria - All Tasks

| Task   | Deliverable                    | Success Criterion                                      |
| ------ | ------------------------------ | ------------------------------------------------------ |
| Task 2 | `PHASE1_API_REFERENCE.md`      | All Phase 1 modules importable + signatures documented |
| Task 4 | `training_ci.yml`              | CI workflow runs successfully on GPU runner            |
| Task 5 | `overhead_analysis.json`       | Overhead < 30% of gross speedup                        |
| Task 1 | `optimization_orchestrator.py` | No parameter conflicts detected + unit tests pass      |
| Task 3 | Test suite                     | >80% coverage, all tests passing                       |

---

## ğŸ¯ Next Steps

1. **Approve this strategy** âœ…
2. **Activate agent subagents** (see next section)
3. **Monitor parallel execution**
4. **Day 1 morning:** Review deliverables + prep for Phase 2 infrastructure creation

---

**STATUS: ğŸŸ¢ Ready for Parallel Agent Execution**
