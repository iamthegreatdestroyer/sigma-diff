"""
Comprehensive Tests for Distributed Inference System

Tests tensor parallelism, orchestration, model loading, and communication.
Validates correctness, performance, and fault tolerance.

Test Coverage:
- Tensor parallel layers (RowParallelLinear, ColumnParallelLinear, ParallelAttention, ParallelMLP)
- Multi-GPU orchestration and synchronization
- Distributed model loading and weight distribution
- Communication patterns and NCCL optimization
- End-to-end distributed inference pipeline
- Performance benchmarks and scaling validation
- Fault tolerance and recovery mechanisms
"""

import pytest
import torch
import torch.nn as nn
import numpy as np
from unittest.mock import Mock, patch
import tempfile
import json
from pathlib import Path
import logging

# Import distributed components
import sys
sys.path.append('RYZEN-LLM')

from src.distributed.tensor_parallel import (
    RowParallelLinear, ColumnParallelLinear,
    ParallelAttention, ParallelMLP,
    TensorParallelTransformerBlock,
    create_tensor_parallel_config,
    validate_tensor_parallel_setup
)
from src.distributed.orchestrator import (
    MultiGPUOrchestrator, ProcessGroupManager,
    DistributedParameterInitializer, GPUPerformanceMonitor
)
from src.distributed.model_loader import (
    DistributedCheckpointLoader, WeightDistributor, CheckpointSaver
)
from src.distributed.communication import NCCLCommunicator
from src.distributed.architecture import DistributedConfig, TensorParallelConfig

logger = logging.getLogger(__name__)


class TestTensorParallelLayers:
    """Test tensor parallel layer implementations."""

    @pytest.fixture
    def mock_communicator(self):
        """Mock NCCL communicator for testing."""
        comm = Mock(spec=NCCLCommunicator)
        comm.world_size = 4
        comm.rank = 0
        comm.all_reduce = Mock()
        comm.all_gather = Mock()
        comm.reduce_scatter = Mock()
        return comm

    @pytest.fixture
    def tensor_config(self):
        """Create tensor parallel configuration."""
        return TensorParallelConfig(
            world_size=4,
            rank=0,
            device=torch.device('cpu'),  # Use CPU for testing
            input_size=1024,
            output_size=4096,
            bias=True
        )

    def test_row_parallel_linear_initialization(self, tensor_config, mock_communicator):
        """Test RowParallelLinear initialization."""
        layer = RowParallelLinear(tensor_config, mock_communicator)

        assert layer.input_size == 1024
        assert layer.output_size == 4096
        assert layer.output_size_per_partition == 1024  # 4096 / 4
        assert layer.weight.shape == (1024, 1024)  # (out_per_partition, in)
        assert layer.bias_param is not None
        assert layer.bias_param.shape == (1024,)

    def test_column_parallel_linear_initialization(self, tensor_config, mock_communicator):
        """Test ColumnParallelLinear initialization."""
        layer = ColumnParallelLinear(tensor_config, mock_communicator)

        assert layer.input_size == 1024
        assert layer.output_size == 4096
        assert layer.input_size_per_partition == 256  # 1024 / 4
        assert layer.weight.shape == (4096, 256)  # (out, in_per_partition)
        assert layer.bias_param is not None
        assert layer.bias_param.shape == (4096,)

    def test_row_parallel_linear_forward(self, tensor_config, mock_communicator):
        """Test RowParallelLinear forward pass."""
        layer = RowParallelLinear(tensor_config, mock_communicator)

        # Mock all_gather to return gathered outputs
        mock_communicator.all_gather.return_value = [
            torch.randn(2, 8, 1024) for _ in range(4)  # 4 ranks
        ]

        input_tensor = torch.randn(2, 8, 1024)  # (batch, seq, hidden)
        output = layer(input_tensor)

        assert output.shape == (2, 8, 4096)  # Full output size
        mock_communicator.all_gather.assert_called_once()

    def test_column_parallel_linear_forward(self, tensor_config, mock_communicator):
        """Test ColumnParallelLinear forward pass."""
        layer = ColumnParallelLinear(tensor_config, mock_communicator)

        input_tensor = torch.randn(2, 8, 1024)  # (batch, seq, hidden)
        output = layer(input_tensor)

        assert output.shape == (2, 8, 4096)  # Full output size

    def test_parallel_attention_initialization(self, mock_communicator):
        """Test ParallelAttention initialization."""
        config = TensorParallelConfig(
            world_size=4, rank=0, device=torch.device('cpu'),
            input_size=1024, output_size=1024, bias=False
        )

        attention = ParallelAttention(config, num_heads=32, head_dim=32, communicator=mock_communicator)

        assert attention.num_attention_heads == 32
        assert attention.num_attention_heads_per_partition == 8  # 32 / 4
        assert attention.head_dim == 32
        assert attention.q_proj is not None
        assert attention.k_proj is not None
        assert attention.v_proj is not None
        assert attention.out_proj is not None

    def test_parallel_mlp_initialization(self, mock_communicator):
        """Test ParallelMLP initialization."""
        config = TensorParallelConfig(
            world_size=4, rank=0, device=torch.device('cpu'),
            input_size=1024, output_size=1024, bias=False
        )

        mlp = ParallelMLP(config, intermediate_size=4096, communicator=mock_communicator)

        assert mlp.hidden_size == 1024
        assert mlp.intermediate_size == 4096
        assert mlp.gate_proj is not None
        assert mlp.up_proj is not None
        assert mlp.down_proj is not None

    def test_tensor_parallel_config_creation(self):
        """Test tensor parallel configuration creation."""
        configs = create_tensor_parallel_config(
            world_size=4, rank=0, device=torch.device('cpu'),
            hidden_size=1024, intermediate_size=4096, num_attention_heads=32
        )

        assert 'attention' in configs
        assert 'mlp' in configs
        assert 'output' in configs

        assert configs['attention'].world_size == 4
        assert configs['attention'].rank == 0
        assert configs['attention'].input_size == 1024
        assert configs['attention'].output_size == 1024

    def test_tensor_parallel_validation(self):
        """Test tensor parallel setup validation."""
        # Valid setup
        assert validate_tensor_parallel_setup(4, 1024, 32) == True

        # Invalid: hidden_size not divisible by world_size
        assert validate_tensor_parallel_setup(4, 1000, 32) == False

        # Invalid: num_heads not divisible by world_size
        assert validate_tensor_parallel_setup(4, 1024, 30) == False


class TestMultiGPUOrchestrator:
    """Test multi-GPU orchestration functionality."""

    @pytest.fixture
    def orchestrator(self):
        """Create test orchestrator."""
        return MultiGPUOrchestrator(
            rank=0, world_size=1,  # Single rank for testing
            backend="gloo",  # Use gloo for CPU testing
            device="cpu",
            enable_monitoring=True
        )

    def test_orchestrator_initialization(self, orchestrator):
        """Test orchestrator initialization."""
        assert orchestrator.rank == 0
        assert orchestrator.world_size == 1
        assert orchestrator.backend == "gloo"
        assert orchestrator.device == torch.device("cpu")
        assert orchestrator.enable_monitoring == True
        assert orchestrator.monitor is not None

    def test_performance_monitor(self):
        """Test GPU performance monitor."""
        monitor = GPUPerformanceMonitor()

        # Test timing
        monitor.start_timer("test_op")
        import time
        time.sleep(0.01)  # Small delay
        duration = monitor.end_timer("test_op")

        assert duration > 0
        assert "test_op" in monitor.metrics

        # Test stats
        stats = monitor.get_stats()
        assert "test_op" in stats
        assert stats["test_op"]["count"] == 1
        assert stats["test_op"]["mean"] > 0

    @patch('torch.distributed.is_initialized', return_value=False)
    def test_process_group_manager(self, mock_is_initialized):
        """Test process group manager."""
        manager = ProcessGroupManager(backend="gloo")

        assert manager.backend == "gloo"
        assert manager._initialized == False
        assert manager.is_initialized() == False

    def test_parameter_initializer(self, orchestrator):
        """Test distributed parameter initializer."""
        initializer = DistributedParameterInitializer(orchestrator)

        # Create simple model
        model = nn.Linear(10, 5)

        # Test parameter broadcasting (would normally use torch.distributed)
        # For testing, just ensure no exceptions
        try:
            initializer.broadcast_parameters(model, src_rank=0)
            initializer.broadcast_buffers(model, src_rank=0)
            assert True  # No exceptions raised
        except Exception as e:
            # In test environment, distributed operations may fail
            # This is expected behavior
            assert "broadcast" in str(e).lower() or "dist" in str(e).lower()


class TestDistributedModelLoading:
    """Test distributed model loading functionality."""

    @pytest.fixture
    def temp_checkpoint_dir(self):
        """Create temporary checkpoint directory."""
        with tempfile.TemporaryDirectory() as tmpdir:
            yield Path(tmpdir)

    def test_weight_distributor_initialization(self):
        """Test weight distributor initialization."""
        distributor = WeightDistributor(rank=0, world_size=4, tp_size=4)

        assert distributor.rank == 0
        assert distributor.world_size == 4
        assert distributor.tp_size == 4
        assert distributor.local_rank == 0

    def test_row_wise_sharding(self):
        """Test row-wise weight sharding."""
        distributor = WeightDistributor(rank=0, world_size=4, tp_size=4)

        # Create test weight matrix
        weight = torch.randn(4096, 1024)  # (out_features, in_features)
        bias = torch.randn(4096)

        sharded_weight, sharded_bias = distributor.shard_linear_layer_row_wise(weight, bias)

        assert sharded_weight.shape == (1024, 1024)  # (4096/4, 1024)
        assert sharded_bias.shape == (1024,)  # (4096/4,)

    def test_column_wise_sharding(self):
        """Test column-wise weight sharding."""
        distributor = WeightDistributor(rank=0, world_size=4, tp_size=4)

        # Create test weight matrix
        weight = torch.randn(1024, 4096)  # (out_features, in_features)
        bias = torch.randn(1024)

        sharded_weight, sharded_bias = distributor.shard_linear_layer_column_wise(weight, bias)

        assert sharded_weight.shape == (1024, 1024)  # (1024, 4096/4)
        assert sharded_bias.shape == (1024,)  # Bias not sharded

    def test_attention_head_sharding(self):
        """Test attention head distribution."""
        distributor = WeightDistributor(rank=0, world_size=4, tp_size=4)

        heads = distributor.shard_attention_heads(num_heads=32)

        assert len(heads) == 8  # 32 / 4
        assert heads == [0, 1, 2, 3, 4, 5, 6, 7]  # First 8 heads for rank 0

    def test_checkpoint_saver(self, temp_checkpoint_dir):
        """Test checkpoint saving functionality."""
        saver = CheckpointSaver(
            checkpoint_dir=str(temp_checkpoint_dir),
            rank=0, world_size=4
        )

        # Test weight saving
        weights = {"layer.weight": torch.randn(10, 5)}
        saver.save_rank_weights(weights, step=100)

        # Check file was created
        weight_file = temp_checkpoint_dir / "weights_rank0_step100.pt"
        assert weight_file.exists()

        # Test metadata saving (rank 0 only)
        metadata = {"step": 100, "loss": 0.5}
        saver.save_metadata(metadata, step=100)

        metadata_file = temp_checkpoint_dir / "metadata_step100.json"
        assert metadata_file.exists()

        # Verify metadata content
        with open(metadata_file, 'r') as f:
            loaded_metadata = json.load(f)
        assert loaded_metadata == metadata


class TestCommunication:
    """Test communication layer functionality."""

    def test_nccl_communicator_initialization(self):
        """Test NCCL communicator initialization."""
        # Note: NCCL requires CUDA, so we test the interface
        try:
            comm = NCCLCommunicator()
            # In test environment, initialization may fail
            # but object should be created
            assert hasattr(comm, 'world_size')
            assert hasattr(comm, 'rank')
        except Exception:
            # Expected in non-CUDA environment
            pass


class TestEndToEndDistributedInference:
    """Test end-to-end distributed inference pipeline."""

    @pytest.fixture
    def distributed_config(self):
        """Create distributed configuration for testing."""
        return DistributedConfig(
            world_size=4,
            rank=0,
            device=torch.device('cpu'),
            hidden_size=1024,
            num_attention_heads=32,
            intermediate_size=4096,
            num_layers=2
        )

    def test_transformer_block_creation(self, distributed_config):
        """Test creation of distributed transformer block."""
        # Mock communicator
        comm = Mock()
        comm.world_size = 4
        comm.rank = 0

        # Create tensor parallel config
        tp_config = TensorParallelConfig(
            world_size=4, rank=0, device=torch.device('cpu'),
            input_size=1024, output_size=1024, bias=False
        )

        # Create transformer block
        block = TensorParallelTransformerBlock(tp_config, num_heads=32, intermediate_size=4096, communicator=comm)

        assert block.attention is not None
        assert block.mlp is not None
        assert block.ln1 is not None
        assert block.ln2 is not None

    def test_forward_pass_correctness(self):
        """Test that distributed forward pass produces correct results."""
        # This would require setting up actual distributed environment
        # For now, test the interface
        comm = Mock()
        comm.world_size = 4
        comm.rank = 0

        config = TensorParallelConfig(
            world_size=4, rank=0, device=torch.device('cpu'),
            input_size=1024, output_size=1024, bias=False
        )

        # Create parallel attention
        attention = ParallelAttention(config, num_heads=32, head_dim=32, communicator=comm)

        # Test input
        batch_size, seq_len = 2, 8
        input_tensor = torch.randn(batch_size, seq_len, 1024)

        # Forward pass (will use mocked communicator)
        output = attention(input_tensor)

        assert output.shape == (batch_size, seq_len, 1024)


class TestPerformanceBenchmarks:
    """Test performance benchmarking functionality."""

    def test_performance_monitor_comprehensive(self):
        """Test comprehensive performance monitoring."""
        monitor = GPUPerformanceMonitor()

        # Simulate operations
        monitor.start_timer("communication")
        import time
        time.sleep(0.01)
        monitor.end_timer("communication")

        monitor.start_timer("computation")
        time.sleep(0.005)
        monitor.end_timer("computation")

        # Simulate memory recording
        monitor.record_memory()

        # Get stats
        stats = monitor.get_stats()

        assert "communication" in stats
        assert "computation" in stats
        assert stats["communication"]["count"] == 1
        assert stats["computation"]["count"] == 1

    def test_orchestrator_performance_stats(self):
        """Test orchestrator performance statistics."""
        orchestrator = MultiGPUOrchestrator(
            rank=0, world_size=1, device="cpu", enable_monitoring=True
        )

        # Simulate some activity
        orchestrator.monitor.record_memory()

        stats = orchestrator.get_performance_stats()

        assert "uptime_seconds" in stats
        assert "failure_count" in stats
        assert "recovery_attempts" in stats


# Integration test for complete distributed pipeline
@pytest.mark.integration
def test_distributed_pipeline_integration():
    """Integration test for complete distributed inference pipeline."""
    # This would require actual multi-GPU setup
    # For now, test component integration

    # Create mock distributed environment
    comm = Mock()
    comm.world_size = 4
    comm.rank = 0

    # Create tensor parallel config
    config = TensorParallelConfig(
        world_size=4, rank=0, device=torch.device('cpu'),
        input_size=1024, output_size=1024, bias=False
    )

    # Create components
    attention = ParallelAttention(config, num_heads=32, head_dim=32, communicator=comm)
    mlp = ParallelMLP(config, intermediate_size=4096, communicator=comm)

    # Test input
    x = torch.randn(2, 8, 1024)

    # Forward pass through components
    attn_out = attention(x)
    mlp_out = mlp(attn_out)

    # Verify shapes
    assert attn_out.shape == (2, 8, 1024)
    assert mlp_out.shape == (2, 8, 1024)

    # Verify outputs are different (non-trivial transformation)
    assert not torch.allclose(attn_out, x, atol=1e-6)
    assert not torch.allclose(mlp_out, attn_out, atol=1e-6)


if __name__ == "__main__":
    # Run basic tests
    logging.basicConfig(level=logging.INFO)

    print("Running distributed inference tests...")

    # Test tensor parallel validation
    assert validate_tensor_parallel_setup(4, 1024, 32) == True
    print("âœ“ Tensor parallel validation passed")

    # Test weight distributor
    distributor = WeightDistributor(rank=0, world_size=4, tp_size=4)
    weight = torch.randn(4096, 1024)
    sharded_weight, _ = distributor.shard_linear_layer_row_wise(weight)
    assert sharded_weight.shape == (1024, 1024)
    print("âœ“ Weight distribution test passed")

    print("All basic tests passed! ðŸŽ‰")