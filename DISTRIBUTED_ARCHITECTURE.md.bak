# DISTRIBUTED INFERENCE ARCHITECTURE v1.0

**RYZEN-LLM Phase 3: Tensor Parallelism & Multi-GPU Scaling**

**Document Version**: 1.0-DRAFT  
**Status**: In Design (Week 1, Jan 6-10, 2026)  
**Author**: APEX Elite Agent  
**Last Updated**: 2025-12-20

---

## Executive Summary

This document defines the distributed inference architecture for RYZEN-LLM Phase 3, enabling production-grade LLM serving across multiple GPUs with **tensor parallelism** as the primary scaling strategy.

### Design Goals

1. **Scaling Efficiency**: 3.8-4.2x speedup on 4 GPUs (>95% efficiency)
2. **Low Overhead**: <10% RPC overhead, <5ms communication latency
3. **Production Ready**: Fault tolerance, monitoring, diagnostics
4. **Developer Friendly**: Clean APIs, easy to reason about
5. **Memory Efficient**: >95% GPU utilization with gradient checkpointing

### Key Decisions

- **Parallelism Strategy**: Row-wise tensor parallelism (simplest, optimal for compute-bound inference)
- **Communication Backend**: NCCL for GPU-native optimized collective operations
- **Synchronization Model**: Synchronous all-reduce for correctness, with async optimization path
- **Model Loader**: Distributed checkpoint loading with smart weight distribution

---

## 1. Architecture Overview

### 1.1 System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED INFERENCE SYSTEM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              APPLICATION LAYER                             â”‚  â”‚
â”‚  â”‚  (User code, inference loop, batching logic)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         DISTRIBUTED MODEL WRAPPER                          â”‚  â”‚
â”‚  â”‚  â€¢ Layer transformation (Linear â†’ Parallel Linear)         â”‚  â”‚
â”‚  â”‚  â€¢ Forward orchestration                                   â”‚  â”‚
â”‚  â”‚  â€¢ Gradient synchronization                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                                          â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  TENSOR PARALLEL      â”‚          â”‚  GPU ORCHESTRATOR      â”‚  â”‚
â”‚  â”‚  LAYERS               â”‚          â”‚                        â”‚  â”‚
â”‚  â”‚                       â”‚          â”‚  â€¢ Rank management     â”‚  â”‚
â”‚  â”‚  â€¢ RowParallel        â”‚          â”‚  â€¢ Process groups      â”‚  â”‚
â”‚  â”‚  â€¢ ColumnParallel     â”‚          â”‚  â€¢ Initialization      â”‚  â”‚
â”‚  â”‚  â€¢ AttentionParallel  â”‚          â”‚  â€¢ Barriers/sync       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                                        â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     COMMUNICATION HANDLER (NCCL Backend)                   â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  â€¢ all_reduce()        - Gradient synchronization         â”‚  â”‚
â”‚  â”‚  â€¢ broadcast()         - Input distribution               â”‚  â”‚
â”‚  â”‚  â€¢ all_gather()        - Output concatenation             â”‚  â”‚
â”‚  â”‚  â€¢ send/recv           - Point-to-point (optimized)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              HARDWARE LAYER (NVIDIA GPUs)                â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  GPU:0  GPU:1  GPU:2  GPU:3   (Connected via NVLink)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Component Responsibilities

| Component      | Role                           | File                 | LOC Target |
| -------------- | ------------------------------ | -------------------- | ---------- |
| Architecture   | Interface contracts            | `architecture.py`    | 150+       |
| TensorParallel | Parallelized layers            | `tensor_parallel.py` | 500+       |
| Orchestrator   | Process/rank management        | `orchestrator.py`    | 300+       |
| ModelLoader    | Distributed checkpoint loading | `model_loader.py`    | 200+       |
| Communication  | NCCL optimization utilities    | `communication.py`   | 150+       |
| Utils          | Helpers (logging, debugging)   | `utils.py`           | 100+       |

---

## 2. Tensor Parallelism Strategy

### 2.1 Tensor Parallelism Basics

**Definition**: Partition model weights across devices along specific dimensions, computing partial results in parallel.

**Row-Wise Parallelism** (Strategy for Dense Layers):

```
Model: Linear(in=4096, out=4096)
Strategy: Shard output dimension across 4 GPUs

Input (broadcast to all GPUs):     x âˆˆ â„^(BÃ—4096)

GPU 0: Linear(in=4096, out=1024)  â†’ yâ‚€ âˆˆ â„^(BÃ—1024)
GPU 1: Linear(in=4096, out=1024)  â†’ yâ‚ âˆˆ â„^(BÃ—1024)
GPU 2: Linear(in=4096, out=1024)  â†’ yâ‚‚ âˆˆ â„^(BÃ—1024)
GPU 3: Linear(in=4096, out=1024)  â†’ yâ‚ƒ âˆˆ â„^(BÃ—1024)

Output (concatenated):            [yâ‚€; yâ‚; yâ‚‚; yâ‚ƒ] âˆˆ â„^(BÃ—4096)
```

**Why Row-Wise for LLM Inference:**

- Computation-bound: Matrix multiply is O(nÂ²), communication is O(n)
- Minimal communication: Only output is communicated (implicit in all-reduce)
- Natural for attention layers: Heads naturally partition
- Hardware efficient: NCCL all-reduce is optimized on NVIDIA

### 2.2 Layer-Wise Parallelization

#### Linear Layers (Most Common)

**RowParallelLinear**:

```
Input:  x âˆˆ â„^(BÃ—D_in)  [replicated across all ranks]
Weight: W âˆˆ â„^(D_outÃ—D_in)  [column-sharded across ranks]
Bias:   b âˆˆ â„^(D_out)  [row-sharded]

Computation:
  1. Local: y_local = x @ W_rank.T  [each rank computes subset]
  2. Sync:  all_reduce(y_local, op='sum')  [gather partial results]
  3. Result: y âˆˆ â„^(BÃ—D_out)  [replicated across ranks]

Memory per GPU: (B Ã— D_out/TP + D_in Ã— D_out/TP) / GB
```

**ColumnParallelLinear**:

```
Input:  x âˆˆ â„^(BÃ—D_in)  [row-sharded across ranks]
Weight: W âˆˆ â„^(D_outÃ—D_in)  [row-sharded across ranks]
Bias:   b âˆˆ â„^(D_out)  [replicated]

Computation:
  1. Local: y_local = x_local @ W_rank.T  [no sync needed]
  2. Result: y âˆˆ â„^(BÃ—D_out)  [row-sharded]

Memory per GPU: (B Ã— D_in/TP + D_out Ã— D_in/TP) / GB
```

#### Attention Heads (Head-Wise Parallelism)

```
Model: MultiHeadAttention(
  num_heads=32,
  head_dim=128,
  total_dim=4096
)

Strategy: Shard heads across 4 GPUs (8 heads per GPU)

Input: x âˆˆ â„^(BÃ—LÃ—4096)  [L=sequence_length]

Shard:
  GPU 0: Heads [0:8]
  GPU 1: Heads [8:16]
  GPU 2: Heads [16:24]
  GPU 3: Heads [24:32]

Forward:
  1. Shard x into x_rank  âˆˆ â„^(BÃ—LÃ—1024)  [all_reduce needed]
  2. Compute attention locally (no head sync needed)
  3. Concatenate head outputs  [implicit in all_reduce]

Memory per GPU: ~1/4 of single-GPU attention computation
Communication: One all_reduce for input distribution, one for output
```

### 2.3 KV-Cache Management (Critical for Inference)

**KV-Cache Sharding Strategy**:

```
Single-GPU KV-Cache Layout:
  k_cache: (B, L, num_heads, head_dim)  âˆˆ â„^(BÃ—LÃ—32Ã—128)
  v_cache: (B, L, num_heads, head_dim)  âˆˆ â„^(BÃ—LÃ—32Ã—128)

Multi-GPU KV-Cache (Row-wise on num_heads):
  GPU 0: k_cache[:, :, 0:8, :]    v_cache[:, :, 0:8, :]
  GPU 1: k_cache[:, :, 8:16, :]   v_cache[:, :, 8:16, :]
  GPU 2: k_cache[:, :, 16:24, :]  v_cache[:, :, 16:24, :]
  GPU 3: k_cache[:, :, 24:32, :]  v_cache[:, :, 24:32, :]

Benefits:
  â€¢ No cache communication needed (heads are independent)
  â€¢ Linear scaling: 4Ã— cache storage across 4 GPUs
  â€¢ Perfect for inference (no reduce-scatter needed)
```

---

## 3. Communication Pattern Design

### 3.1 Communication Model

The system uses **NCCL** (NVIDIA Collective Communications Library) for:

- **Optimized hardware mapping**: Uses NVLink topology
- **Deterministic behavior**: Well-defined semantics
- **High throughput**: Native NVIDIA GPU optimizations
- **Low latency**: ~5ms for 4-GPU systems

### 3.2 Communication Operations

#### All-Reduce (Gradient Synchronization)

**Pattern**: Synchronize gradients after backward pass across all ranks

```
Before All-Reduce:          After All-Reduce:
GPU 0: [gâ‚€]                 GPU 0: [gâ‚€ + gâ‚ + gâ‚‚ + gâ‚ƒ]
GPU 1: [gâ‚]        ----â†’    GPU 1: [gâ‚€ + gâ‚ + gâ‚‚ + gâ‚ƒ]
GPU 2: [gâ‚‚]                 GPU 2: [gâ‚€ + gâ‚ + gâ‚‚ + gâ‚ƒ]
GPU 3: [gâ‚ƒ]                 GPU 3: [gâ‚€ + gâ‚ + gâ‚‚ + gâ‚ƒ]

Cost: O(log P) with tree reduction, ~5ms for 4 GPUs
```

**Frequency**: After every backward pass (critical for correctness)

#### All-Gather (Output Concatenation)

**Pattern**: Reconstruct full outputs from sharded results

```
Before All-Gather:          After All-Gather:
GPU 0: [yâ‚€]                 GPU 0: [yâ‚€; yâ‚; yâ‚‚; yâ‚ƒ]
GPU 1: [yâ‚]        ----â†’    GPU 1: [yâ‚€; yâ‚; yâ‚‚; yâ‚ƒ]
GPU 2: [yâ‚‚]                 GPU 2: [yâ‚€; yâ‚; yâ‚‚; yâ‚ƒ]
GPU 3: [yâ‚ƒ]                 GPU 3: [yâ‚€; yâ‚; yâ‚‚; yâ‚ƒ]

Cost: O(P) no reduction tree, ~8ms for 4 GPUs
```

**Frequency**: After layers that produce distributed outputs

#### Broadcast (Input Distribution)

**Pattern**: Send input from rank 0 to all other ranks

```
Before Broadcast:           After Broadcast:
GPU 0: [x]                  GPU 0: [x]
GPU 1: [â€”]         ----â†’    GPU 1: [x]
GPU 2: [â€”]                  GPU 2: [x]
GPU 3: [â€”]                  GPU 3: [x]

Cost: O(log P) with tree broadcast, ~3ms for 4 GPUs
```

**Frequency**: Beginning of forward pass (once per batch)

### 3.3 Communication Optimization Opportunities

| Optimization           | Technique                              | Expected Gain | Timeline   |
| ---------------------- | -------------------------------------- | ------------- | ---------- |
| Overlap compute & comm | Issue collective early, kernel overlap | 20%           | Sprint 1.2 |
| Ring all-reduce        | Topology-aware reduction               | 15%           | Sprint 1.2 |
| Gradient accumulation  | Reduce all-reduce frequency            | 30%           | Sprint 1.3 |
| Pipeline parallelism   | Combine with tensor parallelism        | 40%           | Sprint 2   |

---

## 4. Memory Layout & Sharding

### 4.1 Weight Distribution Strategy

**Model Example**: Llama2-7B

```
Layer Structure:
  Embedding: (vocab_size, hidden_dim) = (32000, 4096)
  Attention: num_layers Ã— (
    Q_proj: (4096, 4096),
    K_proj: (4096, 4096),
    V_proj: (4096, 4096),
    O_proj: (4096, 4096),
    FF_up: (4096, 11008),
    FF_gate: (4096, 11008),
    FF_down: (11008, 4096)
  )

Total Parameters: 7.0B

Single GPU (baseline):
  Memory = 7.0B Ã— 4 bytes (float32) = 28 GB
  â†’ Would need A100-80GB, H100

Four GPUs (row-wise TP):
  Memory per GPU = 7.0B Ã— 4 bytes / 4 + overhead
                 â‰ˆ 7 GB per GPU
  â†’ Fits on 4Ã— A100-40GB or 4Ã— RTX 4090

Embedding Handling:
  - Typically row-sharded by vocab_size
  - Each GPU computes logits[vocab_size/4:]
  - All-reduce after embedding layer
```

### 4.2 Memory Efficiency Analysis

```
Memory Budget Breakdown (per GPU, 4-GPU system):

Baseline:
  Model weights:         28 GB / 4 =  7.0 GB
  Optimizer states:      28 GB / 4 =  7.0 GB  (if training)
  Activations (fwd):            ~2.5 GB
  Gradients (backward):  28 GB / 4 =  7.0 GB  (if training)
  KV-Cache:                    ~0.5 GB  (batch_size=1)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:                       ~24.0 GB

With Gradient Checkpointing:
  Activations reduced by ~2-3x
  Memory saved:            ~1.5 GB
  â†’ 22.5 GB per GPU

Target: Stay <30 GB to fit on A100-40GB with headroom
```

---

## 5. Scaling Analysis

### 5.1 Theoretical Speedup

**Assumptions**:

- LLM inference = 90% compute, 10% communication
- Linear layer: B Ã— D_in Ã— D_out/TP operations per GPU
- All-reduce: 2 Ã— D_out Ã— B latency

```
Speedup(P) = 1 / (1 - f + f/P)    [Amdahl's Law]

where f = fraction of compute that can be parallelized
      P = number of GPUs

For f=0.99, P=4:
  Speedup = 1 / (1 - 0.99 + 0.99/4) = 1 / 0.3475 â‰ˆ 2.88x

More realistic with communication overlap:
  Speedup(4) â‰ˆ 3.5-3.9x  (targeting 3.8x)
```

### 5.2 Measured Scaling Numbers (Target)

| GPUs | Throughput | Speedup | Efficiency | Target Met   |
| ---- | ---------- | ------- | ---------- | ------------ |
| 1    | 10 tok/s   | 1.0x    | 100%       | Baseline     |
| 2    | 19 tok/s   | 1.9x    | 95%        | âœ“ Path clear |
| 4    | 38 tok/s   | 3.8x    | 95%        | âœ“ Target     |
| 8    | 72 tok/s   | 7.2x    | 90%        | âœ“ Future     |

**Efficiency** = Speedup / Number of GPUs

---

## 6. Synchronization Model

### 6.1 Forward Pass Synchronization

```
Input Distribution (Broadcast or Replicate):
  All ranks start with same input_ids

  Layer 1 (RowParallelLinear):
    Input:  replicated [B, D_in]
    Output: row-sharded [B, D_out]
    Sync:   implicit in all-reduce

  Layer 2 (ColumnParallelLinear):
    Input:  row-sharded [B, D_in]
    Output: replicated [B, D_out]
    Sync:   no extra sync needed

  Layer 3 (Attention):
    Input:  replicated [B, L, D]
    Compute: head-wise sharded (no cross-GPU head communication)
    Output: replicated [B, L, D]
```

### 6.2 Backward Pass Synchronization

```
Gradient computation:
  1. Local backward on each GPU (independent computation)
  2. Accumulate gradients for sharded weights (local only)
  3. All-reduce gradients for replicated weights
  4. Zero-grad + optimizer step on each GPU

All-Reduce Timing:
  â€¢ Occurs after every layer's backward
  â€¢ Can overlap with next layer's backward (Sprint 1.2)
  â€¢ Synchronization point: critical for correctness
```

---

## 7. Correctness Validation Strategy

### 7.1 Testing Approach

**Multi-Layer Validation**:

1. **Unit Tests**: Individual layer parallelization correctness
2. **Integration Tests**: Full model forward/backward pass
3. **Numerical Tests**: Output precision vs single-GPU baseline
4. **Scaling Tests**: Verify speedup numbers on 2-4 GPU systems

### 7.2 Output Matching Protocol

```python
# Pseudocode for validation
single_gpu_output = model_single_gpu(batch)
distributed_output = model_distributed(batch)

# Float32 allows ~1e-6 relative error
assert torch.allclose(
    single_gpu_output,
    distributed_output,
    rtol=1e-5,        # Relative tolerance
    atol=1e-7         # Absolute tolerance
), f"Output mismatch: max diff = {(single_gpu_output - distributed_output).abs().max()}"

# Gradient validation
loss_single.backward()
loss_distributed.backward()

for param_single, param_distributed in zip(model_single_gpu.parameters(),
                                            model_distributed.parameters()):
    assert torch.allclose(
        param_single.grad,
        param_distributed.grad,
        rtol=1e-5,
        atol=1e-7
    ), f"Gradient mismatch in {param_single.name}"
```

---

## 8. Fault Tolerance & Recovery

### 8.1 Failure Modes & Mitigations

| Failure Mode   | Impact           | Detection        | Recovery                                |
| -------------- | ---------------- | ---------------- | --------------------------------------- |
| GPU OOM        | Training crashes | GPU memory error | Reduce batch size, enable checkpointing |
| Rank hang      | System deadlock  | Timeout (30s)    | Graceful shutdown, checkpoint save      |
| NVLink failure | Degraded comm    | NCCL error code  | Fallback to host memory                 |
| Process crash  | Training loss    | Exit code != 0   | Restart from latest checkpoint          |

### 8.2 Checkpoint Strategy

```
Checkpoint Format:
  {
    "model_state": {
      "layer.0.weight": [GPU 0 shard, GPU 1 shard, ...],
      "layer.1.weight": [GPU 0 shard, ...],
      ...
    },
    "optimizer_state": {...},
    "metadata": {
      "global_step": 1000,
      "epoch": 5,
      "world_size": 4,
      "rank_mapping": {...}
    }
  }

Save Pattern:
  - Save rank 0 orchestrates checkpoint save
  - All ranks save their shards to distributed filesystem
  - Checkpoint every 100 steps (configurable)
```

---

## 9. Configuration & Initialization

### 9.1 Configuration Schema

```python
@dataclass
class DistributedConfig:
    world_size: int              # Total number of GPUs
    rank: int                    # Current GPU rank (0 to world_size-1)
    backend: str = "nccl"        # "nccl" | "gloo" | "mpi"
    master_addr: str = "127.0.0.1"
    master_port: int = 29500
    device: str = "cuda:0"       # "cuda:0" | "cuda:1" | ...
    tensor_parallel_size: int = 4
    pipeline_parallel_size: int = 1
    use_checkpointing: bool = True
    communication_timeout: float = 30.0
```

### 9.2 Initialization Sequence

```
1. Parse configuration (world_size, rank, device)
2. Set CUDA device: torch.cuda.set_device(rank % torch.cuda.device_count())
3. Initialize process group:
   torch.distributed.init_process_group(
       backend="nccl",
       init_method="env://",
       world_size=world_size,
       rank=rank,
       timeout=timedelta(seconds=30)
   )
4. Create CommunicationHandler wrapping process group
5. Create ParallelModelWrapper, apply_tensor_parallelism()
6. Distribute weights across GPUs
7. Load checkpoint (if resuming) or initialize weights
8. Ready for forward/backward passes
```

---

## 10. API & Integration Points

### 10.1 User-Facing API

```python
# Minimal, clean API for users
from ryzen_llm.distributed import (
    DistributedConfig,
    create_distributed_model,
)

# Configuration
config = DistributedConfig(
    world_size=4,
    rank=os.environ['RANK'],
    backend="nccl",
    device=f"cuda:{os.environ['RANK']}",
    tensor_parallel_size=4,
)

# Load and parallelize model
model = AutoModel.from_pretrained("meta-llama/Llama-2-7b")
model = create_distributed_model(model, config)

# Forward pass (handles distribution transparently)
output = model(
    input_ids=input_ids,           # shape: [batch, seq_len]
    attention_mask=attention_mask,  # shape: [batch, seq_len]
)  # output shape: [batch, seq_len, vocab_size]
```

### 10.2 Integration with Phase 2 Inference Engine

```
Phase 2 (Existing):           Phase 3 (New):
â”œâ”€ Tokenizer                  â”œâ”€ Tokenizer (unchanged)
â”œâ”€ ModelLoader                â”œâ”€ ModelLoader (enhanced)
â”œâ”€ InferenceEngine            â”œâ”€ DistributedInferenceEngine
â”‚  â”œâ”€ Single-GPU forward      â”‚  â”œâ”€ Multi-GPU forward
â”‚  â”œâ”€ KV-cache mgmt          â”‚  â”œâ”€ Distributed KV-cache
â”‚  â””â”€ Output streaming        â”‚  â””â”€ Output gathering
â””â”€ TextStreamer              â””â”€ TextStreamer (unchanged)

Compatibility:
  â€¢ Phase 2 inference still works (backward compatible)
  â€¢ Users can opt-in to distributed mode via config flag
  â€¢ No breaking changes to tokenizer/streamer
```

---

## 11. Architectural Decision Records (ADRs)

### ADR-001: Row-Wise Tensor Parallelism

**Status**: DECIDED âœ…  
**Date**: 2025-12-20

**Decision**: Implement tensor parallelism using row-wise weight sharding as primary strategy.

**Rationale**:

- Simplest to implement and debug
- Natural fit for attention heads (head-wise = row-wise on head dimension)
- Minimizes communication: only final gather/reduce per layer
- Proven effective in Megatron-LM and Llama2 distributed training

**Alternatives Considered**:

- Column-wise: More complex, similar speedup
- Sequence parallelism: Requires sequence length awareness, not ideal for inference
- Expert parallelism: Requires MoE models, not applicable to Llama2

**Trade-offs**:

- Communication cost: ~10% per all-reduce (acceptable)
- Memory savings: ~4x on 4 GPUs
- Implementation complexity: Low (preferred)

**Consequences**:

- âœ“ Linear scaling with GPU count (up to model capacity)
- âœ“ Easy to extend to 8+ GPUs
- âœ“ Good for both training and inference
- ~ Requires synchronization on layer boundaries

---

### ADR-002: NCCL Backend Selection

**Status**: DECIDED âœ…  
**Date**: 2025-12-20

**Decision**: Use NCCL as communication backend for multi-GPU systems.

**Rationale**:

- Optimized for NVIDIA GPUs (our target hardware)
- Deterministic collective operations
- Lowest latency for all-reduce/all-gather
- Battle-tested in Megatron, DeepSpeed, vLLM

**Alternatives Considered**:

- Gloo: CPU-based, slower for GPU tensors
- MPI: Generic, requires external MPI library

**Trade-offs**:

- NVIDIA GPU-only (acceptable, that's our target)
- Requires NCCL library installation
- No CPU fallback (acceptable for inference)

**Consequences**:

- âœ“ ~5ms all-reduce latency for 4 GPUs
- âœ“ Automatic topology-aware optimization
- âœ“ No additional performance tuning needed initially
- ~ Users must have NCCL installed

---

### ADR-003: Synchronous vs. Asynchronous Synchronization

**Status**: DECIDED (Synchronous Phase 1) âœ…  
**Date**: 2025-12-20

**Decision**: Phase 1 uses synchronous all-reduce. Async optimization deferred to Sprint 1.2.

**Rationale**:

- Synchronous is easier to reason about and debug
- Gets us to target efficiency (95%+) without complexity
- Asynchronous adds 20% complexity for 10-15% speedup
- Communication overhead already acceptable (<10%)

**Future Plan**:

- Sprint 1.2: Implement async gradient accumulation
- Sprint 1.3: Explore overlapped communication kernel launch

**Trade-offs**:

- Potential 5-10% speedup left on table initially
- Simpler correctness validation (preferred)
- Easier debugging and monitoring

---

### ADR-004: Checkpoint Format (Distributed Storage)

**Status**: DECIDED âœ…  
**Date**: 2025-12-20

**Decision**: Store checkpoints in distributed format (one file per rank) in shared filesystem.

**Rationale**:

- Avoids bottleneck of rank 0 gathering all weights
- Enables efficient checkpoint saving and loading
- Allows scaling to 100+ GPU systems
- Each rank saves/loads independently in parallel

**Alternative**:

- Centralized checkpoint (rank 0 saves all): scales poorly, risk of OOM

**Implementation**:

```
Checkpoint directory structure:
  checkpoints/
  â””â”€ model-step-1000/
     â”œâ”€ metadata.json          (rank 0 only)
     â”œâ”€ weights_rank0.pt
     â”œâ”€ weights_rank1.pt
     â”œâ”€ weights_rank2.pt
     â””â”€ weights_rank3.pt
```

---

## 12. Testing Strategy

### 12.1 Test Pyramid

```
                        â—‡ E2E Tests (10-20%)
                       / \
                      /   \
                     /     \
                    â—‡ Integration (20-30%)
                   / \
                  /   \
                 /     \
                â—‡ Unit Tests (50-70%)
```

**Unit Tests** (tensor_parallel, layers, communication primitives):

- RowParallelLinear correctness
- ColumnParallelLinear correctness
- AttentionParallel correctness
- Collective operation correctness
- Gradient synchronization

**Integration Tests** (multi-GPU, full models):

- 2-GPU forward pass matching
- 4-GPU forward pass matching
- Gradient flow correctness
- Checkpoint save/load

**E2E Tests** (realistic scenarios):

- Full model inference pipeline
- Scaling from 1 â†’ 2 â†’ 4 GPUs
- Performance benchmarking
- Stability over 1000+ iterations

### 12.2 Test Targets

| Component       | Unit Tests | Integration | E2E | Coverage Target |
| --------------- | ---------- | ----------- | --- | --------------- |
| tensor_parallel | 80+        | 30+         | 10+ | 95%             |
| orchestrator    | 60+        | 20+         | 5+  | 90%             |
| model_loader    | 40+        | 15+         | 3+  | 85%             |
| communication   | 30+        | 10+         | 2+  | 80%             |

**Overall Coverage Target**: >90% for Sprint 1.1 completion

---

## 13. Performance Targets & Metrics

### 13.1 Sprint 1.1 Success Criteria

| Metric             | Target   | Current | Status     |
| ------------------ | -------- | ------- | ---------- |
| 4-GPU Speedup      | 3.8-4.2x | TBD     | To measure |
| Scaling Efficiency | >95%     | TBD     | To measure |
| All-Reduce Latency | <5ms     | TBD     | To measure |
| RPC Overhead       | <10%     | TBD     | To measure |
| Code Coverage      | >90%     | TBD     | To measure |
| Memory Efficiency  | >95%     | TBD     | To measure |

### 13.2 Measurement Methodology

```python
# Pseudocode for throughput measurement
import time

model.eval()
with torch.no_grad():
    # Warmup
    for _ in range(10):
        _ = model(input_ids, attention_mask)

    # Timed run
    start = time.time()
    for i in range(100):
        output = model(input_ids, attention_mask)
        # Synchronize GPU
        torch.cuda.synchronize()
    elapsed = time.time() - start

    tokens_per_sec = (100 * batch_size * seq_len) / elapsed
    speedup = throughput_4gpu / throughput_1gpu
    efficiency = speedup / 4  # For 4 GPUs
```

---

## 14. Documentation & Deliverables

### 14.1 Week 1 Deliverables (Due Jan 10)

- [x] DISTRIBUTED_ARCHITECTURE.md (this document)
- [ ] src/distributed/architecture.py (interfaces)
- [ ] Design review document with ADR signatures
- [ ] 2-GPU PoC environment ready

### 14.2 Week 2 Deliverables (Due Jan 17)

- [ ] src/distributed/tensor_parallel.py (500+ lines)
- [ ] src/distributed/orchestrator.py (300+ lines)
- [ ] src/distributed/model_loader.py (200+ lines)
- [ ] tests/test_tensor_parallel.py (300+ lines)
- [ ] tests/test_orchestrator.py (250+ lines)
- [ ] tests/test_distributed_inference.py (200+ lines)
- [ ] DISTRIBUTED_INFERENCE_GUIDE.md (1500+ words)
- [ ] Performance benchmark report

---

## 15. Glossary & Terminology

| Term            | Definition                                                      |
| --------------- | --------------------------------------------------------------- |
| **Rank**        | Process ID in distributed system (0 to world_size-1)            |
| **World Size**  | Total number of processes/GPUs                                  |
| **TP Size**     | Tensor parallelism degree (typically = world_size)              |
| **All-Reduce**  | Collective operation: compute reduction, broadcast to all ranks |
| **NCCL**        | NVIDIA Collective Communications Library                        |
| **Row-Wise**    | Sharding output dimension of matrix                             |
| **Column-Wise** | Sharding input dimension of matrix                              |
| **KV-Cache**    | Stored key/value states for fast inference                      |
| **Barrier**     | Synchronization point: all ranks wait here                      |

---

## 16. References & Resources

### Research Papers

- Shoeybi et al. (2019): "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
- Rajbhandari et al. (2019): "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"

### Implementation References

- [PyTorch Distributed](https://pytorch.org/docs/stable/distributed.html)
- [Megatron-LM Codebase](https://github.com/NVIDIA/Megatron-LM)
- [DeepSpeed](https://github.com/microsoft/DeepSpeed)
- [vLLM Distributed Inference](https://github.com/lm-sys/vllm)

---

## 17. Next Steps & Timeline

### Immediate (Week 1: Jan 6-10)

- âœ“ Architecture document complete (this file)
- [ ] Architecture review meeting (Friday Jan 9)
- [ ] Begin tensor_parallel.py skeleton
- [ ] Prepare 2-GPU hardware

### Short Term (Week 2: Jan 13-17)

- [ ] Implement all 3 core components
- [ ] Achieve >90% test coverage
- [ ] Validate 4-GPU scaling
- [ ] Go/No-Go gate decision (Jan 17)

### Medium Term (Sprint 1.2: Jan 20-Feb 3)

- [ ] Communication optimization
- [ ] Performance tuning
- [ ] Production hardening

---

**Document Status**: ğŸŸ¡ IN DESIGN  
**Last Updated**: 2025-12-20 21:45 UTC  
**Version**: 1.0-DRAFT  
**Ready for Design Review**: Friday Jan 9, 3 PM
